--- models/huggingface_transformers/src/transformers/models/deformable_detr/modeling_deformable_detr.py.ori	2023-12-14 20:49:38.053997818 +0000
+++ models/huggingface_transformers/src/transformers/models/deformable_detr/modeling_deformable_detr.py.opt_1	2023-12-14 20:49:38.057997809 +0000
@@ -685,9 +685,8 @@
         attention_weights = self.attention_weights(hidden_states).view(
             batch_size, num_queries, self.n_heads, self.n_levels * self.n_points
         )
-        attention_weights = F.softmax(attention_weights, -1).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
-        )
+        from xformers.triton.softmax import softmax as softmax_softmax
+        attention_weights = softmax_softmax(attention_weights).view(batch_size, num_queries, self.n_heads, self.n_levels, self.n_points)
         # batch_size, num_queries, n_heads, n_levels, n_points, 2
         if reference_points.shape[-1] == 2:
             offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)

--- models/huggingface_transformers/src/transformers/models/mistral/modeling_mistral.py.ori	2023-12-14 20:52:53.753562377 +0000
+++ models/huggingface_transformers/src/transformers/models/mistral/modeling_mistral.py.opt_1	2023-12-14 20:52:53.757562368 +0000
@@ -297,7 +297,8 @@
             attn_weights = attn_weights + attention_mask
 
         # upcast attention to fp32
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
+        from xformers.triton.softmax import softmax as softmax_softmax
+        attn_weights = softmax_softmax(attn_weights).to(query_states.dtype)
         attn_output = torch.matmul(attn_weights, value_states)
 
         if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):

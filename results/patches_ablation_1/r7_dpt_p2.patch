--- models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py.ori	2023-12-14 21:20:25.810054611 +0000
+++ models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py.opt_2	2023-12-14 21:20:25.814054602 +0000
@@ -1197,13 +1197,8 @@
         super().__init__()
 
         features = config.fusion_hidden_size
-        self.head = nn.Sequential(
-            nn.Conv2d(features, features, kernel_size=3, padding=1, bias=False),
-            nn.BatchNorm2d(features),
-            ACT2FN["relu"],
-            nn.Dropout(0.1, False),
-            nn.Conv2d(features, config.num_labels, kernel_size=1),
-        )
+        from epoi.ops.torchscript_ops import FusedConv2dBatchNorm2d as torchscript_ops_FusedConv2dBatchNorm2d
+        self.head = nn.Sequential(torchscript_ops_FusedConv2dBatchNorm2d(features, features, 3, bias=False), ACT2FN['relu'], nn.Dropout(0.1, False), nn.Conv2d(features, config.num_labels, kernel_size=1))
 
     def forward(self, hidden_states):
         logits = self.head(hidden_states)

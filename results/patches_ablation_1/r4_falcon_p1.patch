--- models/huggingface_transformers/src/transformers/models/falcon/modeling_falcon.py.ori	2023-12-14 20:50:19.773904114 +0000
+++ models/huggingface_transformers/src/transformers/models/falcon/modeling_falcon.py.opt_1	2023-12-14 20:50:19.773904114 +0000
@@ -489,9 +489,8 @@
                 attention_scores = query_layer_ @ key_layer_.transpose(-1, -2)
                 attention_scores /= math.sqrt(self.head_dim)
 
-                attention_scores = F.softmax(
-                    attention_scores + attention_mask_float, dim=-1, dtype=hidden_states.dtype
-                )
+                from xformers.triton.softmax import softmax as softmax_softmax
+                attention_scores = softmax_softmax((attention_scores + attention_mask_float))
                 attn_output = attention_scores @ value_layer_
 
             attn_output = attn_output.view(batch_size, self.num_heads, query_length, self.head_dim)

--- models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py.ori	2023-12-14 20:34:46.027870009 +0000
+++ models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py.opt_2	2023-12-14 20:34:46.031870000 +0000
@@ -616,7 +616,8 @@
 class T5LayerCrossAttention(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention
+        self.EncDecAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=False, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 

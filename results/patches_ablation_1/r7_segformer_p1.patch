--- models/huggingface_transformers/src/transformers/models/segformer/modeling_segformer.py.ori	2023-12-14 21:25:28.229429487 +0000
+++ models/huggingface_transformers/src/transformers/models/segformer/modeling_segformer.py.opt_1	2023-12-14 21:25:28.233429479 +0000
@@ -689,13 +689,8 @@
         self.linear_c = nn.ModuleList(mlps)
 
         # the following 3 layers implement the ConvModule of the original implementation
-        self.linear_fuse = nn.Conv2d(
-            in_channels=config.decoder_hidden_size * config.num_encoder_blocks,
-            out_channels=config.decoder_hidden_size,
-            kernel_size=1,
-            bias=False,
-        )
-        self.batch_norm = nn.BatchNorm2d(config.decoder_hidden_size)
+        from epoi.ops.torchscript_ops import FusedConv2dBatchNorm2d as torchscript_ops_FusedConv2dBatchNorm2d
+        self.conv_batchnorm = torchscript_ops_FusedConv2dBatchNorm2d((config.decoder_hidden_size * config.num_encoder_blocks), config.decoder_hidden_size, 1, bias=False)
         self.activation = nn.ReLU()
 
         self.dropout = nn.Dropout(config.classifier_dropout_prob)
@@ -725,8 +720,7 @@
             )
             all_hidden_states += (encoder_hidden_state,)
 
-        hidden_states = self.linear_fuse(torch.cat(all_hidden_states[::-1], dim=1))
-        hidden_states = self.batch_norm(hidden_states)
+        hidden_states = self.conv_batchnorm(torch.cat(all_hidden_states[::(- 1)], dim=1))
         hidden_states = self.activation(hidden_states)
         hidden_states = self.dropout(hidden_states)
 

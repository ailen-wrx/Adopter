--- models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py.ori	2023-12-14 20:56:51.637044080 +0000
+++ models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py.opt_2	2023-12-14 20:56:51.637044080 +0000
@@ -1645,7 +1645,8 @@
         logits = self.classifier(hidden_states)
 
         loss = None
-        if labels is not None:
+        from xformers.triton.softmax import softmax as softmax_softmax
+        norm_weights = softmax_softmax(self.layer_weights)
             loss_fct = CrossEntropyLoss()
             loss = loss_fct(logits.view(-1, self.num_labels), torch.argmax(labels.view(-1, self.num_labels), axis=1))
 

--- models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py.ori	2023-12-14 20:40:26.847169498 +0000
+++ models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py.opt_1	2023-12-14 20:40:26.851169489 +0000
@@ -365,7 +365,8 @@
         inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size
 
         self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
-        self.attn = GPT2Attention(config, layer_idx=layer_idx)
+        from epoi.ops.xformers_attn import GPT2AttentionWithXF as xformers_attn_GPT2AttentionWithXF
+        self.attn = xformers_attn_GPT2AttentionWithXF(config, layer_idx=layer_idx)
         self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
 
         if config.add_cross_attention:

--- models/huggingface_transformers/src/transformers/models/distilbert/modeling_distilbert.py.ori	2023-12-14 20:49:57.893953191 +0000
+++ models/huggingface_transformers/src/transformers/models/distilbert/modeling_distilbert.py.opt_1	2023-12-14 20:49:57.893953191 +0000
@@ -223,7 +223,8 @@
             mask, torch.tensor(torch.finfo(scores.dtype).min)
         )  # (bs, n_heads, q_length, k_length)
 
-        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)
+        from xformers.triton.softmax import softmax as softmax_softmax
+        weights = softmax_softmax(scores)
         weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)
 
         # Mask heads if we want to

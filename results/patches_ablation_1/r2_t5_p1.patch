--- models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py.ori	2023-12-14 20:34:45.967870131 +0000
+++ models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py.opt_1	2023-12-14 20:34:45.967870131 +0000
@@ -584,7 +584,8 @@
 class T5LayerSelfAttention(nn.Module):
     def __init__(self, config, has_relative_attention_bias=False):
         super().__init__()
-        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention
+        self.SelfAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=has_relative_attention_bias, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 

--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
@@ -194,7 +194,7 @@
                 # Apply the attention mask is (precomputed for all layers in NystromformerModel forward() function)
                 attention_scores = attention_scores + attention_mask
 
-            attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
             context_layer = torch.matmul(attention_probs, value_layer)
 
         else:
@@ -222,7 +222,7 @@
                 # Apply the attention mask is (precomputed for all layers in NystromformerModel forward() function)
                 attention_scores = attention_scores + attention_mask
 
-            kernel_3 = nn.functional.softmax(attention_scores, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; kernel_3 = softmax_softmax(attention_scores)
             attention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2))
             new_value_layer = torch.matmul(kernel_3, value_layer)
             context_layer = torch.matmul(attention_probs, new_value_layer)

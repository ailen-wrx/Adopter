--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py
@@ -584,7 +584,7 @@
 class T5LayerSelfAttention(nn.Module):
     def __init__(self, config, has_relative_attention_bias=False):
         super().__init__()
-        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention; self.SelfAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=has_relative_attention_bias, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 
@@ -616,7 +616,7 @@
 class T5LayerCrossAttention(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention; self.EncDecAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=False, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 

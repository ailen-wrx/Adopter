--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vits/modeling_vits.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vits/modeling_vits.py
@@ -268,7 +268,7 @@
     if min_bin_height * num_bins > 1.0:
         raise ValueError(f"Minimal bin height {min_bin_height} too large for the number of bins {num_bins}")
 
-    widths = nn.functional.softmax(unnormalized_widths, dim=-1)
+    from xformers.triton.softmax import softmax as softmax_softmax; widths = softmax_softmax(unnormalized_widths)
     widths = min_bin_width + (1 - min_bin_width * num_bins) * widths
     cumwidths = torch.cumsum(widths, dim=-1)
     cumwidths = nn.functional.pad(cumwidths, pad=(1, 0), mode="constant", value=0.0)
@@ -279,7 +279,7 @@
 
     derivatives = min_derivative + nn.functional.softplus(unnormalized_derivatives)
 
-    heights = nn.functional.softmax(unnormalized_heights, dim=-1)
+    from xformers.triton.softmax import softmax as softmax_softmax; heights = softmax_softmax(unnormalized_heights)
     heights = min_bin_height + (1 - min_bin_height * num_bins) * heights
     cumheights = torch.cumsum(heights, dim=-1)
     cumheights = nn.functional.pad(cumheights, pad=(1, 0), mode="constant", value=0.0)
@@ -961,7 +961,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):

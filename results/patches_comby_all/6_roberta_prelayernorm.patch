--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
@@ -358,11 +358,7 @@
     def __init__(self, config):
         super().__init__()
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.LayerNorm(hidden_states)

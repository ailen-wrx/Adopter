--- models/huggingface_transformers/src/transformers/models/llama/modeling_llama.py.ori	2023-11-21 18:20:18.397556739 +0000
+++ models/huggingface_transformers/src/transformers/models/llama/modeling_llama.py.opt_1	2023-11-21 18:20:18.397556739 +0000
@@ -384,7 +384,8 @@
             attn_weights = attn_weights + attention_mask
 
         # upcast attention to fp32
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
+        from xformers.triton.softmax import softmax as softmax_softmax
+        attn_weights = softmax_softmax(attn_weights).to(query_states.dtype)
         attn_output = torch.matmul(attn_weights, value_states)
 
         if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):

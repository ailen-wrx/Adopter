--- models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py.ori	2023-11-21 18:20:20.253553381 +0000
+++ models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py.opt_2	2023-11-21 18:20:20.257553374 +0000
@@ -1089,9 +1089,8 @@
         global_attn_scores = global_attn_scores.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
 
         # compute global attn probs
-        global_attn_probs_float = nn.functional.softmax(
-            global_attn_scores, dim=-1, dtype=torch.float32
-        )  # use fp32 for numerical stability
+        from xformers.triton.softmax import softmax as softmax_softmax
+        global_attn_probs_float = softmax_softmax(global_attn_scores)
 
         # apply layer head masking
         if layer_head_mask is not None:

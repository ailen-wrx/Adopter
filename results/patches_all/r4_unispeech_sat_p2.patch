--- models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py.ori	2023-11-21 18:21:50.761389556 +0000
+++ models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py.opt_2	2023-11-21 18:21:50.761389556 +0000
@@ -1642,7 +1642,8 @@
         hidden_states = self.projector(hidden_states)
         if attention_mask is None:
             pooled_output = hidden_states.mean(dim=1)
-        else:
+        from xformers.triton.softmax import softmax as softmax_softmax
+        norm_weights = softmax_softmax(self.layer_weights)
             padding_mask = self._get_feature_vector_attention_mask(hidden_states.shape[1], attention_mask)
             hidden_states[~padding_mask] = 0.0
             pooled_output = hidden_states.sum(dim=1) / padding_mask.sum(dim=1).view(-1, 1)

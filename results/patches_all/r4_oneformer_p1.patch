--- models/huggingface_transformers/src/transformers/models/oneformer/modeling_oneformer.py.ori	2023-11-21 18:20:57.901485254 +0000
+++ models/huggingface_transformers/src/transformers/models/oneformer/modeling_oneformer.py.opt_1	2023-11-21 18:20:57.905485246 +0000
@@ -1027,9 +1027,8 @@
         attention_weights = self.attention_weights(hidden_states).view(
             batch_size, num_queries, self.n_heads, self.n_levels * self.n_points
         )
-        attention_weights = nn.functional.softmax(attention_weights, -1).view(
-            batch_size, num_queries, self.n_heads, self.n_levels, self.n_points
-        )
+        from xformers.triton.softmax import softmax as softmax_softmax
+        attention_weights = softmax_softmax(attention_weights).view(batch_size, num_queries, self.n_heads, self.n_levels, self.n_points)
         # batch_size, num_queries, n_heads, n_levels, n_points, 2
         if reference_points.shape[-1] == 2:
             offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)

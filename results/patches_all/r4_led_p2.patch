--- models/huggingface_transformers/src/transformers/models/led/modeling_led.py.ori	2023-11-21 18:20:16.029561023 +0000
+++ models/huggingface_transformers/src/transformers/models/led/modeling_led.py.opt_2	2023-11-21 18:20:16.033561016 +0000
@@ -720,9 +720,8 @@
         global_attn_scores = global_attn_scores.view(batch_size * self.num_heads, max_num_global_attn_indices, seq_len)
 
         # compute global attn probs
-        global_attn_probs_float = nn.functional.softmax(
-            global_attn_scores, dim=-1, dtype=torch.float32
-        )  # use fp32 for numerical stability
+        from xformers.triton.softmax import softmax as softmax_softmax
+        global_attn_probs_float = softmax_softmax(global_attn_scores)
 
         # apply layer head masking
         if layer_head_mask is not None:

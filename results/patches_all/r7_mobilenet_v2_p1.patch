--- models/huggingface_transformers/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py.ori	2023-11-21 18:33:46.344091004 +0000
+++ models/huggingface_transformers/src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py.opt_1	2023-11-21 18:33:46.344091004 +0000
@@ -310,29 +310,14 @@
 
         padding = 0 if config.tf_padding else int((kernel_size - 1) / 2) * dilation
 
-        self.convolution = nn.Conv2d(
-            in_channels=in_channels,
-            out_channels=out_channels,
-            kernel_size=kernel_size,
-            stride=stride,
-            padding=padding,
-            dilation=dilation,
-            groups=groups,
-            bias=bias,
-            padding_mode="zeros",
-        )
-
         if use_normalization:
-            self.normalization = nn.BatchNorm2d(
-                num_features=out_channels,
-                eps=config.layer_norm_eps if layer_norm_eps is None else layer_norm_eps,
-                momentum=0.997,
-                affine=True,
-                track_running_stats=True,
-            )
+            from epoi.ops.torchscript_ops import FusedConv2dBatchNorm2d as torchscript_ops_FusedConv2dBatchNorm2d
+            self.conv_batchnorm = torchscript_ops_FusedConv2dBatchNorm2d(in_channels, out_channels, kernel_size, stride=stride, bias=bias)
         else:
+            self.convolution = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode='zeros')
             self.normalization = None
 
+
         if use_activation:
             if isinstance(use_activation, str):
                 self.activation = ACT2FN[use_activation]
@@ -346,9 +331,11 @@
     def forward(self, features: torch.Tensor) -> torch.Tensor:
         if self.config.tf_padding:
             features = apply_tf_padding(features, self.convolution)
-        features = self.convolution(features)
-        if self.normalization is not None:
-            features = self.normalization(features)
+        if (self.normalization is not None):
+            features = self.conv_batchnorm(features)
+        else:
+            features = self.convolution(features)
+        features = self.normalization(features)
         if self.activation is not None:
             features = self.activation(features)
         return features

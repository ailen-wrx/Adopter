[RULE 1]
[RULE 1][MODEL 1]
[RULE 1][MODEL 2]
[RULE 1][MODEL 3]
[RULE 1][MODEL 4]
[RULE 1][MODEL 5]
[RULE 1][MODEL 6]
[RULE 1][MODEL 7]
[RULE 1][MODEL 8]
[RULE 1][MODEL 9]
[RULE 1][MODEL 10]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
@@ -392,7 +392,7 @@
 class BertAttention(nn.Module):
     def __init__(self, config, position_embedding_type=None):
         super().__init__()
-        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)
+        from epoi.ops.xformers_attn import BertSelfAttention as xformers_attn_BertSelfAttention; self.self = xformers_attn_BertSelfAttention(config, position_embedding_type=position_embedding_type, attn_op_name="cutlass")
         self.output = BertSelfOutput(config)
         self.pruned_heads = set()
 
[RULE 1][MODEL 11]
[RULE 1][MODEL 12]
[RULE 1][MODEL 13]
[RULE 1][MODEL 14]
[RULE 1][MODEL 15]
[RULE 1][MODEL 16]
[RULE 1][MODEL 17]
[RULE 1][MODEL 18]
[RULE 1][MODEL 19]
[RULE 1][MODEL 20]
[RULE 1][MODEL 21]
[RULE 1][MODEL 22]
[RULE 1][MODEL 23]
[RULE 1][MODEL 24]
[RULE 1][MODEL 25]
[RULE 1][MODEL 26]
[RULE 1][MODEL 27]
[RULE 1][MODEL 28]
[RULE 1][MODEL 29]
[RULE 1][MODEL 30]
[RULE 1][MODEL 31]
[RULE 1][MODEL 32]
[RULE 1][MODEL 33]
[RULE 1][MODEL 34]
[RULE 1][MODEL 35]
[RULE 1][MODEL 36]
[RULE 1][MODEL 37]
[RULE 1][MODEL 38]
[RULE 1][MODEL 39]
[RULE 1][MODEL 40]
[RULE 1][MODEL 41]
[RULE 1][MODEL 42]
[RULE 1][MODEL 43]
[RULE 1][MODEL 44]
[RULE 1][MODEL 45]
[RULE 1][MODEL 46]
[RULE 1][MODEL 47]
[RULE 1][MODEL 48]
[RULE 1][MODEL 49]
[RULE 1][MODEL 50]
[RULE 1][MODEL 51]
[RULE 1][MODEL 52]
[RULE 1][MODEL 53]
[RULE 1][MODEL 54]
[RULE 1][MODEL 55]
[RULE 1][MODEL 56]
[RULE 1][MODEL 57]
[RULE 1][MODEL 58]
[RULE 1][MODEL 59]
[RULE 1][MODEL 60]
[RULE 1][MODEL 61]
[RULE 1][MODEL 62]
[RULE 1][MODEL 63]
[RULE 1][MODEL 64]
[RULE 1][MODEL 65]
[RULE 1][MODEL 66]
[RULE 1][MODEL 67]
[RULE 1][MODEL 68]
[RULE 1][MODEL 69]
[RULE 1][MODEL 70]
[RULE 1][MODEL 71]
[RULE 1][MODEL 72]
[RULE 1][MODEL 73]
[RULE 1][MODEL 74]
[RULE 1][MODEL 75]
[RULE 1][MODEL 76]
[RULE 1][MODEL 77]
[RULE 1][MODEL 78]
[RULE 1][MODEL 79]
[RULE 1][MODEL 80]
[RULE 1][MODEL 81]
[RULE 1][MODEL 82]
[RULE 1][MODEL 83]
[RULE 1][MODEL 84]
[RULE 1][MODEL 85]
[RULE 1][MODEL 86]
[RULE 1][MODEL 87]
[RULE 1][MODEL 88]
[RULE 1][MODEL 89]
[RULE 1][MODEL 90]
[RULE 1][MODEL 91]
[RULE 1][MODEL 92]
[RULE 1][MODEL 93]
[RULE 1][MODEL 94]
[RULE 1][MODEL 95]
[RULE 1][MODEL 96]
[RULE 1][MODEL 97]
[RULE 1][MODEL 98]
[RULE 1][MODEL 99]
[RULE 1][MODEL 100]
[RULE 1][MODEL 101]
[RULE 1][MODEL 102]
[RULE 1][MODEL 103]
[RULE 1][MODEL 104]
[RULE 1][MODEL 105]
[RULE 1][MODEL 106]
[RULE 1][MODEL 107]
[RULE 1][MODEL 108]
[RULE 1][MODEL 109]
[RULE 1][MODEL 110]
[RULE 1][MODEL 111]
[RULE 1][MODEL 112]
[RULE 1][MODEL 113]
[RULE 1][MODEL 114]
[RULE 1][MODEL 115]
[RULE 1][MODEL 116]
[RULE 1][MODEL 117]
[RULE 1][MODEL 118]
[RULE 1][MODEL 119]
[RULE 1][MODEL 120]
[RULE 1][MODEL 121]
[RULE 1][MODEL 122]
[RULE 1][MODEL 123]
[RULE 1][MODEL 124]
[RULE 1][MODEL 125]
[RULE 1][MODEL 126]
[RULE 1][MODEL 127]
[RULE 1][MODEL 128]
[RULE 1][MODEL 129]
[RULE 1][MODEL 130]
[RULE 1][MODEL 131]
[RULE 1][MODEL 132]
[RULE 1][MODEL 133]
[RULE 1][MODEL 134]
[RULE 1][MODEL 135]
[RULE 1][MODEL 136]
[RULE 1][MODEL 137]
[RULE 1][MODEL 138]
[RULE 1][MODEL 139]
[RULE 1][MODEL 140]
[RULE 1][MODEL 141]
[RULE 1][MODEL 142]
[RULE 1][MODEL 143]
[RULE 1][MODEL 144]
[RULE 1][MODEL 145]
[RULE 1][MODEL 146]
[RULE 1][MODEL 147]
[RULE 1][MODEL 148]
[RULE 1][MODEL 149]
[RULE 1][MODEL 150]
[RULE 1][MODEL 151]
[RULE 1][MODEL 152]
[RULE 1][MODEL 153]
[RULE 1][MODEL 154]
[RULE 1][MODEL 155]
[RULE 1][MODEL 156]
[RULE 1][MODEL 157]
[RULE 1][MODEL 158]
[RULE 1][MODEL 159]
[RULE 1][MODEL 160]
[RULE 1][MODEL 161]
[RULE 1][MODEL 162]
[RULE 1][MODEL 163]
[RULE 1][MODEL 164]
[RULE 1][MODEL 165]
[RULE 1][MODEL 166]
[RULE 1][MODEL 167]
[RULE 1][MODEL 168]
[RULE 1][MODEL 169]
[RULE 1][MODEL 170]
[RULE 1][MODEL 171]
[RULE 1][MODEL 172]
[RULE 1][MODEL 173]
[RULE 1][MODEL 174]
[RULE 1][MODEL 175]
[RULE 1][MODEL 176]
[RULE 1][MODEL 177]
[RULE 1][MODEL 178]
[RULE 1][MODEL 179]
[RULE 1][MODEL 180]
[RULE 1][MODEL 181]
[RULE 1][MODEL 182]
[RULE 1][MODEL 183]
[RULE 1][MODEL 184]
[RULE 1][MODEL 185]
[RULE 1][MODEL 186]
[RULE 1][MODEL 187]
[RULE 1][MODEL 188]
[RULE 1][MODEL 189]
[RULE 1][MODEL 190]
[RULE 1][MODEL 191]
[RULE 1][MODEL 192]
[RULE 1][MODEL 193]
[RULE 1][MODEL 194]
[RULE 1][MODEL 195]
[RULE 1][MODEL 196]
[RULE 1][MODEL 197]
[RULE 1][MODEL 198]
[RULE 1][MODEL 199]
[RULE 2]
[RULE 2][MODEL 1]
[RULE 2][MODEL 2]
[RULE 2][MODEL 3]
[RULE 2][MODEL 4]
[RULE 2][MODEL 5]
[RULE 2][MODEL 6]
[RULE 2][MODEL 7]
[RULE 2][MODEL 8]
[RULE 2][MODEL 9]
[RULE 2][MODEL 10]
[RULE 2][MODEL 11]
[RULE 2][MODEL 12]
[RULE 2][MODEL 13]
[RULE 2][MODEL 14]
[RULE 2][MODEL 15]
[RULE 2][MODEL 16]
[RULE 2][MODEL 17]
[RULE 2][MODEL 18]
[RULE 2][MODEL 19]
[RULE 2][MODEL 20]
[RULE 2][MODEL 21]
[RULE 2][MODEL 22]
[RULE 2][MODEL 23]
[RULE 2][MODEL 24]
[RULE 2][MODEL 25]
[RULE 2][MODEL 26]
[RULE 2][MODEL 27]
[RULE 2][MODEL 28]
[RULE 2][MODEL 29]
[RULE 2][MODEL 30]
[RULE 2][MODEL 31]
[RULE 2][MODEL 32]
[RULE 2][MODEL 33]
[RULE 2][MODEL 34]
[RULE 2][MODEL 35]
[RULE 2][MODEL 36]
[RULE 2][MODEL 37]
[RULE 2][MODEL 38]
[RULE 2][MODEL 39]
[RULE 2][MODEL 40]
[RULE 2][MODEL 41]
[RULE 2][MODEL 42]
[RULE 2][MODEL 43]
[RULE 2][MODEL 44]
[RULE 2][MODEL 45]
[RULE 2][MODEL 46]
[RULE 2][MODEL 47]
[RULE 2][MODEL 48]
[RULE 2][MODEL 49]
[RULE 2][MODEL 50]
[RULE 2][MODEL 51]
[RULE 2][MODEL 52]
[RULE 2][MODEL 53]
[RULE 2][MODEL 54]
[RULE 2][MODEL 55]
[RULE 2][MODEL 56]
[RULE 2][MODEL 57]
[RULE 2][MODEL 58]
[RULE 2][MODEL 59]
[RULE 2][MODEL 60]
[RULE 2][MODEL 61]
[RULE 2][MODEL 62]
[RULE 2][MODEL 63]
[RULE 2][MODEL 64]
[RULE 2][MODEL 65]
[RULE 2][MODEL 66]
[RULE 2][MODEL 67]
[RULE 2][MODEL 68]
[RULE 2][MODEL 69]
[RULE 2][MODEL 70]
[RULE 2][MODEL 71]
[RULE 2][MODEL 72]
[RULE 2][MODEL 73]
[RULE 2][MODEL 74]
[RULE 2][MODEL 75]
[RULE 2][MODEL 76]
[RULE 2][MODEL 77]
[RULE 2][MODEL 78]
[RULE 2][MODEL 79]
[RULE 2][MODEL 80]
[RULE 2][MODEL 81]
[RULE 2][MODEL 82]
[RULE 2][MODEL 83]
[RULE 2][MODEL 84]
[RULE 2][MODEL 85]
[RULE 2][MODEL 86]
[RULE 2][MODEL 87]
[RULE 2][MODEL 88]
[RULE 2][MODEL 89]
[RULE 2][MODEL 90]
[RULE 2][MODEL 91]
[RULE 2][MODEL 92]
[RULE 2][MODEL 93]
[RULE 2][MODEL 94]
[RULE 2][MODEL 95]
[RULE 2][MODEL 96]
[RULE 2][MODEL 97]
[RULE 2][MODEL 98]
[RULE 2][MODEL 99]
[RULE 2][MODEL 100]
[RULE 2][MODEL 101]
[RULE 2][MODEL 102]
[RULE 2][MODEL 103]
[RULE 2][MODEL 104]
[RULE 2][MODEL 105]
[RULE 2][MODEL 106]
[RULE 2][MODEL 107]
[RULE 2][MODEL 108]
[RULE 2][MODEL 109]
[RULE 2][MODEL 110]
[RULE 2][MODEL 111]
[RULE 2][MODEL 112]
[RULE 2][MODEL 113]
[RULE 2][MODEL 114]
[RULE 2][MODEL 115]
[RULE 2][MODEL 116]
[RULE 2][MODEL 117]
[RULE 2][MODEL 118]
[RULE 2][MODEL 119]
[RULE 2][MODEL 120]
[RULE 2][MODEL 121]
[RULE 2][MODEL 122]
[RULE 2][MODEL 123]
[RULE 2][MODEL 124]
[RULE 2][MODEL 125]
[RULE 2][MODEL 126]
[RULE 2][MODEL 127]
[RULE 2][MODEL 128]
[RULE 2][MODEL 129]
[RULE 2][MODEL 130]
[RULE 2][MODEL 131]
[RULE 2][MODEL 132]
[RULE 2][MODEL 133]
[RULE 2][MODEL 134]
[RULE 2][MODEL 135]
[RULE 2][MODEL 136]
[RULE 2][MODEL 137]
[RULE 2][MODEL 138]
[RULE 2][MODEL 139]
[RULE 2][MODEL 140]
[RULE 2][MODEL 141]
[RULE 2][MODEL 142]
[RULE 2][MODEL 143]
[RULE 2][MODEL 144]
[RULE 2][MODEL 145]
[RULE 2][MODEL 146]
[RULE 2][MODEL 147]
[RULE 2][MODEL 148]
[RULE 2][MODEL 149]
[RULE 2][MODEL 150]
[RULE 2][MODEL 151]
[RULE 2][MODEL 152]
[RULE 2][MODEL 153]
[RULE 2][MODEL 154]
[RULE 2][MODEL 155]
[RULE 2][MODEL 156]
[RULE 2][MODEL 157]
[RULE 2][MODEL 158]
[RULE 2][MODEL 159]
[RULE 2][MODEL 160]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/t5/modeling_t5.py
@@ -584,7 +584,7 @@
 class T5LayerSelfAttention(nn.Module):
     def __init__(self, config, has_relative_attention_bias=False):
         super().__init__()
-        self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention; self.SelfAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=has_relative_attention_bias, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 
@@ -616,7 +616,7 @@
 class T5LayerCrossAttention(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)
+        from epoi.ops.xformers_attn import T5Attention as xformers_attn_T5Attention; self.EncDecAttention = xformers_attn_T5Attention(config, has_relative_attention_bias=False, attn_op_name="cutlass")
         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
         self.dropout = nn.Dropout(config.dropout_rate)
 
[RULE 2][MODEL 161]
[RULE 2][MODEL 162]
[RULE 2][MODEL 163]
[RULE 2][MODEL 164]
[RULE 2][MODEL 165]
[RULE 2][MODEL 166]
[RULE 2][MODEL 167]
[RULE 2][MODEL 168]
[RULE 2][MODEL 169]
[RULE 2][MODEL 170]
[RULE 2][MODEL 171]
[RULE 2][MODEL 172]
[RULE 2][MODEL 173]
[RULE 2][MODEL 174]
[RULE 2][MODEL 175]
[RULE 2][MODEL 176]
[RULE 2][MODEL 177]
[RULE 2][MODEL 178]
[RULE 2][MODEL 179]
[RULE 2][MODEL 180]
[RULE 2][MODEL 181]
[RULE 2][MODEL 182]
[RULE 2][MODEL 183]
[RULE 2][MODEL 184]
[RULE 2][MODEL 185]
[RULE 2][MODEL 186]
[RULE 2][MODEL 187]
[RULE 2][MODEL 188]
[RULE 2][MODEL 189]
[RULE 2][MODEL 190]
[RULE 2][MODEL 191]
[RULE 2][MODEL 192]
[RULE 2][MODEL 193]
[RULE 2][MODEL 194]
[RULE 2][MODEL 195]
[RULE 2][MODEL 196]
[RULE 2][MODEL 197]
[RULE 2][MODEL 198]
[RULE 2][MODEL 199]
[RULE 3]
[RULE 3][MODEL 1]
[RULE 3][MODEL 2]
[RULE 3][MODEL 3]
[RULE 3][MODEL 4]
[RULE 3][MODEL 5]
[RULE 3][MODEL 6]
[RULE 3][MODEL 7]
[RULE 3][MODEL 8]
[RULE 3][MODEL 9]
[RULE 3][MODEL 10]
[RULE 3][MODEL 11]
[RULE 3][MODEL 12]
[RULE 3][MODEL 13]
[RULE 3][MODEL 14]
[RULE 3][MODEL 15]
[RULE 3][MODEL 16]
[RULE 3][MODEL 17]
[RULE 3][MODEL 18]
[RULE 3][MODEL 19]
[RULE 3][MODEL 20]
[RULE 3][MODEL 21]
[RULE 3][MODEL 22]
[RULE 3][MODEL 23]
[RULE 3][MODEL 24]
[RULE 3][MODEL 25]
[RULE 3][MODEL 26]
[RULE 3][MODEL 27]
[RULE 3][MODEL 28]
[RULE 3][MODEL 29]
[RULE 3][MODEL 30]
[RULE 3][MODEL 31]
[RULE 3][MODEL 32]
[RULE 3][MODEL 33]
[RULE 3][MODEL 34]
[RULE 3][MODEL 35]
[RULE 3][MODEL 36]
[RULE 3][MODEL 37]
[RULE 3][MODEL 38]
[RULE 3][MODEL 39]
[RULE 3][MODEL 40]
[RULE 3][MODEL 41]
[RULE 3][MODEL 42]
[RULE 3][MODEL 43]
[RULE 3][MODEL 44]
[RULE 3][MODEL 45]
[RULE 3][MODEL 46]
[RULE 3][MODEL 47]
[RULE 3][MODEL 48]
[RULE 3][MODEL 49]
[RULE 3][MODEL 50]
[RULE 3][MODEL 51]
[RULE 3][MODEL 52]
[RULE 3][MODEL 53]
[RULE 3][MODEL 54]
[RULE 3][MODEL 55]
[RULE 3][MODEL 56]
[RULE 3][MODEL 57]
[RULE 3][MODEL 58]
[RULE 3][MODEL 59]
[RULE 3][MODEL 60]
[RULE 3][MODEL 61]
[RULE 3][MODEL 62]
[RULE 3][MODEL 63]
[RULE 3][MODEL 64]
[RULE 3][MODEL 65]
[RULE 3][MODEL 66]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py
@@ -369,7 +369,7 @@
         self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
 
         if config.add_cross_attention:
-            self.crossattention = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)
+            from epoi.ops.xformers_attn import GPT2Attention as xformers_attn_GPT2Attention; self.crossattention = xformers_attn_T5Attention(config, is_cross_attention=True, layer_idx=layer_idx, attn_op_name="cutlass")
             self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
 
         self.mlp = GPT2MLP(inner_dim, config)
[RULE 3][MODEL 67]
[RULE 3][MODEL 68]
[RULE 3][MODEL 69]
[RULE 3][MODEL 70]
[RULE 3][MODEL 71]
[RULE 3][MODEL 72]
[RULE 3][MODEL 73]
[RULE 3][MODEL 74]
[RULE 3][MODEL 75]
[RULE 3][MODEL 76]
[RULE 3][MODEL 77]
[RULE 3][MODEL 78]
[RULE 3][MODEL 79]
[RULE 3][MODEL 80]
[RULE 3][MODEL 81]
[RULE 3][MODEL 82]
[RULE 3][MODEL 83]
[RULE 3][MODEL 84]
[RULE 3][MODEL 85]
[RULE 3][MODEL 86]
[RULE 3][MODEL 87]
[RULE 3][MODEL 88]
[RULE 3][MODEL 89]
[RULE 3][MODEL 90]
[RULE 3][MODEL 91]
[RULE 3][MODEL 92]
[RULE 3][MODEL 93]
[RULE 3][MODEL 94]
[RULE 3][MODEL 95]
[RULE 3][MODEL 96]
[RULE 3][MODEL 97]
[RULE 3][MODEL 98]
[RULE 3][MODEL 99]
[RULE 3][MODEL 100]
[RULE 3][MODEL 101]
[RULE 3][MODEL 102]
[RULE 3][MODEL 103]
[RULE 3][MODEL 104]
[RULE 3][MODEL 105]
[RULE 3][MODEL 106]
[RULE 3][MODEL 107]
[RULE 3][MODEL 108]
[RULE 3][MODEL 109]
[RULE 3][MODEL 110]
[RULE 3][MODEL 111]
[RULE 3][MODEL 112]
[RULE 3][MODEL 113]
[RULE 3][MODEL 114]
[RULE 3][MODEL 115]
[RULE 3][MODEL 116]
[RULE 3][MODEL 117]
[RULE 3][MODEL 118]
[RULE 3][MODEL 119]
[RULE 3][MODEL 120]
[RULE 3][MODEL 121]
[RULE 3][MODEL 122]
[RULE 3][MODEL 123]
[RULE 3][MODEL 124]
[RULE 3][MODEL 125]
[RULE 3][MODEL 126]
[RULE 3][MODEL 127]
[RULE 3][MODEL 128]
[RULE 3][MODEL 129]
[RULE 3][MODEL 130]
[RULE 3][MODEL 131]
[RULE 3][MODEL 132]
[RULE 3][MODEL 133]
[RULE 3][MODEL 134]
[RULE 3][MODEL 135]
[RULE 3][MODEL 136]
[RULE 3][MODEL 137]
[RULE 3][MODEL 138]
[RULE 3][MODEL 139]
[RULE 3][MODEL 140]
[RULE 3][MODEL 141]
[RULE 3][MODEL 142]
[RULE 3][MODEL 143]
[RULE 3][MODEL 144]
[RULE 3][MODEL 145]
[RULE 3][MODEL 146]
[RULE 3][MODEL 147]
[RULE 3][MODEL 148]
[RULE 3][MODEL 149]
[RULE 3][MODEL 150]
[RULE 3][MODEL 151]
[RULE 3][MODEL 152]
[RULE 3][MODEL 153]
[RULE 3][MODEL 154]
[RULE 3][MODEL 155]
[RULE 3][MODEL 156]
[RULE 3][MODEL 157]
[RULE 3][MODEL 158]
[RULE 3][MODEL 159]
[RULE 3][MODEL 160]
[RULE 3][MODEL 161]
[RULE 3][MODEL 162]
[RULE 3][MODEL 163]
[RULE 3][MODEL 164]
[RULE 3][MODEL 165]
[RULE 3][MODEL 166]
[RULE 3][MODEL 167]
[RULE 3][MODEL 168]
[RULE 3][MODEL 169]
[RULE 3][MODEL 170]
[RULE 3][MODEL 171]
[RULE 3][MODEL 172]
[RULE 3][MODEL 173]
[RULE 3][MODEL 174]
[RULE 3][MODEL 175]
[RULE 3][MODEL 176]
[RULE 3][MODEL 177]
[RULE 3][MODEL 178]
[RULE 3][MODEL 179]
[RULE 3][MODEL 180]
[RULE 3][MODEL 181]
[RULE 3][MODEL 182]
[RULE 3][MODEL 183]
[RULE 3][MODEL 184]
[RULE 3][MODEL 185]
[RULE 3][MODEL 186]
[RULE 3][MODEL 187]
[RULE 3][MODEL 188]
[RULE 3][MODEL 189]
[RULE 3][MODEL 190]
[RULE 3][MODEL 191]
[RULE 3][MODEL 192]
[RULE 3][MODEL 193]
[RULE 3][MODEL 194]
[RULE 3][MODEL 195]
[RULE 3][MODEL 196]
[RULE 3][MODEL 197]
[RULE 3][MODEL 198]
[RULE 3][MODEL 199]
[RULE 4]
[RULE 4][MODEL 1]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/albert/modeling_albert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/albert/modeling_albert.py
@@ -352,7 +352,7 @@
                 attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 2]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
@@ -848,7 +848,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 3]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
@@ -399,7 +399,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -794,7 +794,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 4]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py
@@ -154,7 +154,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 5]
[RULE 4][MODEL 6]
[RULE 4][MODEL 7]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bark/modeling_bark.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bark/modeling_bark.py
@@ -129,7 +129,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
         attn_weights = attn_weights.to(value.dtype)
         attn_weights = self.attn_dropout(attn_weights)
 
[RULE 4][MODEL 8]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bart/modeling_bart.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bart/modeling_bart.py
@@ -250,7 +250,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 9]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/beit/modeling_beit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/beit/modeling_beit.py
@@ -262,7 +262,7 @@
             attention_scores = attention_scores + relative_position_bias
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 10]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
@@ -352,7 +352,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 11]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
@@ -169,7 +169,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 12]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
@@ -391,7 +391,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -826,7 +826,7 @@
         last_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -1], key_layer, ndim=4)
         last_product = last_product * rsqrt_d
         last_product += (1.0 - to_mask) * attn_mask_penalty
-        last_attn_weights = nn.functional.softmax(last_product, dim=-1)  # [bsz, n_heads, from_block_size, n]
+        from xformers.triton.softmax import softmax as softmax_softmax; last_attn_weights = softmax_softmax(last_product)  # [bsz, n_heads, from_block_size, n]
 
         # [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]
         last_context_layer = self.torch_bmm_nd(last_attn_weights, value_layer, ndim=4)
[RULE 4][MODEL 13]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py
@@ -208,7 +208,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -644,7 +644,7 @@
         last_product = self.torch_bmm_nd_transpose(blocked_query_matrix[:, :, -1], key_layer, ndim=4)
         last_product = last_product * rsqrt_d
         last_product += (1.0 - to_mask) * attn_mask_penalty
-        last_attn_weights = nn.functional.softmax(last_product, dim=-1)  # [bsz, n_heads, from_block_size, n]
+        from xformers.triton.softmax import softmax as softmax_softmax; last_attn_weights = softmax_softmax(last_product)  # [bsz, n_heads, from_block_size, n]
 
         # [bsz, n_heads, from_block_size, to_seq_len] x [bsz, n_heads, to_seq_len, -1] ==> [bsz, n_heads, from_block_size, -1]
         last_context_layer = self.torch_bmm_nd(last_attn_weights, value_layer, ndim=4)
@@ -1317,7 +1317,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 14]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/biogpt/modeling_biogpt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/biogpt/modeling_biogpt.py
@@ -223,7 +223,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 15]
[RULE 4][MODEL 16]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blenderbot/modeling_blenderbot.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blenderbot/modeling_blenderbot.py
@@ -237,7 +237,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 17]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blenderbot_small/modeling_blenderbot_small.py
@@ -234,7 +234,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 18]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip/modeling_blip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip/modeling_blip.py
@@ -336,7 +336,7 @@
         attention_scores = attention_scores * self.scale
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 19]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
@@ -177,7 +177,7 @@
         attention_scores = attention_scores * self.scale
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 20]
[RULE 4][MODEL 21]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
@@ -542,7 +542,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 22]
[RULE 4][MODEL 23]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
@@ -277,7 +277,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 24]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
@@ -478,7 +478,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 25]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
@@ -318,7 +318,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -459,7 +459,7 @@
                 f" {attn_weights.size()}"
             )
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 26]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
@@ -475,7 +475,7 @@
             attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -1348,7 +1348,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 27]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clip/modeling_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clip/modeling_clip.py
@@ -308,7 +308,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 28]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clipseg/modeling_clipseg.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clipseg/modeling_clipseg.py
@@ -328,7 +328,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 29]
[RULE 4][MODEL 30]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/conditional_detr/modeling_conditional_detr.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/conditional_detr/modeling_conditional_detr.py
@@ -674,7 +674,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
@@ -791,7 +791,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 31]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
@@ -397,7 +397,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 32]
[RULE 4][MODEL 33]
[RULE 4][MODEL 34]
[RULE 4][MODEL 35]
[RULE 4][MODEL 36]
[RULE 4][MODEL 37]
[RULE 4][MODEL 38]
[RULE 4][MODEL 39]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/decision_transformer/modeling_decision_transformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/decision_transformer/modeling_decision_transformer.py
@@ -192,7 +192,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise
         attn_weights = attn_weights.type(value.dtype)
@@ -242,7 +242,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise
         if attn_weights.dtype != torch.float32:
[RULE 4][MODEL 40]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deformable_detr/modeling_deformable_detr.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deformable_detr/modeling_deformable_detr.py
@@ -811,7 +811,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 41]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deit/modeling_deit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deit/modeling_deit.py
@@ -175,7 +175,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 42]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deta/modeling_deta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deta/modeling_deta.py
@@ -697,7 +697,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 43]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/detr/modeling_detr.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/detr/modeling_detr.py
@@ -638,7 +638,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 44]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dinat/modeling_dinat.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dinat/modeling_dinat.py
@@ -351,7 +351,7 @@
         attention_scores = natten2dqkrpb(query_layer, key_layer, self.rpb, self.kernel_size, self.dilation)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 45]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dinov2/modeling_dinov2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dinov2/modeling_dinov2.py
@@ -208,7 +208,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 46]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/distilbert/modeling_distilbert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/distilbert/modeling_distilbert.py
@@ -223,7 +223,7 @@
             mask, torch.tensor(torch.finfo(scores.dtype).min)
         )  # (bs, n_heads, q_length, k_length)
 
-        weights = nn.functional.softmax(scores, dim=-1)  # (bs, n_heads, q_length, k_length)
+        from xformers.triton.softmax import softmax as softmax_softmax; weights = softmax_softmax(scores)  # (bs, n_heads, q_length, k_length)
         weights = self.dropout(weights)  # (bs, n_heads, q_length, k_length)
 
         # Mask heads if we want to
[RULE 4][MODEL 47]
[RULE 4][MODEL 48]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py
@@ -345,7 +345,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 49]
[RULE 4][MODEL 50]
[RULE 4][MODEL 51]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
@@ -324,7 +324,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 52]
[RULE 4][MODEL 53]
[RULE 4][MODEL 54]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
@@ -259,7 +259,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 55]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie_m/modeling_ernie_m.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie_m/modeling_ernie_m.py
@@ -205,7 +205,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 56]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/esm/modeling_esm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/esm/modeling_esm.py
@@ -367,7 +367,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 57]
[RULE 4][MODEL 58]
[RULE 4][MODEL 59]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
@@ -475,9 +475,9 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 60]
[RULE 4][MODEL 61]
[RULE 4][MODEL 62]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fsmt/modeling_fsmt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fsmt/modeling_fsmt.py
@@ -957,7 +957,7 @@
             attn_weights = attn_weights.masked_fill(reshaped, torch.finfo(attn_weights.dtype).min)
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             assert layer_head_mask.size() == (
[RULE 4][MODEL 63]
[RULE 4][MODEL 64]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
@@ -246,7 +246,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
@@ -728,7 +728,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 65]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/glpn/modeling_glpn.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/glpn/modeling_glpn.py
@@ -179,7 +179,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 66]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt2/modeling_gpt2.py
@@ -205,7 +205,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op otherwise
         attn_weights = attn_weights.type(value.dtype)
@@ -255,7 +255,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         # Downcast (if necessary) back to V's dtype (if in mixed-precision) -- No-Op if otherwise
         if attn_weights.dtype != torch.float32:
[RULE 4][MODEL 67]
[RULE 4][MODEL 68]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
@@ -200,7 +200,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
         attn_weights = attn_weights.to(value.dtype)
         attn_weights = self.attn_dropout(attn_weights)
 
[RULE 4][MODEL 69]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py
@@ -268,7 +268,7 @@
             # Apply the attention mask
             attn_scores = attn_scores + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_scores)
         attn_weights = attn_weights.to(value.dtype)
 
         # Mask heads if we want to
[RULE 4][MODEL 70]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py
@@ -226,7 +226,7 @@
             # Apply the attention mask
             attn_scores = attn_scores + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_scores)
         attn_weights = self.attention_dropout(attn_weights)
         attn_weights = attn_weights.to(value.dtype)
 
[RULE 4][MODEL 71]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
@@ -172,7 +172,7 @@
             # Apply the attention mask
             attn_weights = attn_weights + attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
         attn_weights = attn_weights.to(value.dtype)
         attn_weights = self.attn_dropout(attn_weights)
 
[RULE 4][MODEL 72]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptsan_japanese/modeling_gptsan_japanese.py
@@ -471,7 +471,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 73]
[RULE 4][MODEL 74]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/groupvit/modeling_groupvit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/groupvit/modeling_groupvit.py
@@ -678,7 +678,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 75]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
@@ -504,7 +504,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 76]
[RULE 4][MODEL 77]
[RULE 4][MODEL 78]
[RULE 4][MODEL 79]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/informer/modeling_informer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/informer/modeling_informer.py
@@ -420,7 +420,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
@@ -620,7 +620,7 @@
             )
             attn_weights = attn_weights.view(bsz * self.num_heads, u, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 80]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
@@ -179,7 +179,7 @@
         attention_scores = attention_scores * self.scale
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 81]
[RULE 4][MODEL 82]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
@@ -240,7 +240,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 83]
[RULE 4][MODEL 84]
[RULE 4][MODEL 85]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/led/modeling_led.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/led/modeling_led.py
@@ -896,7 +896,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
                 raise ValueError(
[RULE 4][MODEL 86]
[RULE 4][MODEL 87]
[RULE 4][MODEL 88]
[RULE 4][MODEL 89]
[RULE 4][MODEL 90]
[RULE 4][MODEL 91]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
@@ -585,7 +585,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 92]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lxmert/modeling_lxmert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lxmert/modeling_lxmert.py
@@ -355,7 +355,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 93]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/m2m_100/modeling_m2m_100.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/m2m_100/modeling_m2m_100.py
@@ -305,7 +305,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 94]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/marian/modeling_marian.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/marian/modeling_marian.py
@@ -252,7 +252,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 95]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
@@ -447,7 +447,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 96]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mask2former/modeling_mask2former.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mask2former/modeling_mask2former.py
@@ -1509,7 +1509,7 @@
                 )
             attn_weights += attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 97]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/maskformer/modeling_maskformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/maskformer/modeling_maskformer.py
@@ -548,7 +548,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 98]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mbart/modeling_mbart.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mbart/modeling_mbart.py
@@ -246,7 +246,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 99]
[RULE 4][MODEL 100]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/megatron_bert/modeling_megatron_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/megatron_bert/modeling_megatron_bert.py
@@ -302,7 +302,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 101]
[RULE 4][MODEL 102]
[RULE 4][MODEL 103]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilebert/modeling_mobilebert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilebert/modeling_mobilebert.py
@@ -289,7 +289,7 @@
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
         attention_probs = self.dropout(attention_probs)
[RULE 4][MODEL 104]
[RULE 4][MODEL 105]
[RULE 4][MODEL 106]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilevit/modeling_mobilevit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilevit/modeling_mobilevit.py
@@ -257,7 +257,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 107]
[RULE 4][MODEL 108]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
@@ -185,7 +185,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         attention_probs = self.dropout(attention_probs)
 
[RULE 4][MODEL 109]
[RULE 4][MODEL 110]
[RULE 4][MODEL 111]
[RULE 4][MODEL 112]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/musicgen/modeling_musicgen.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/musicgen/modeling_musicgen.py
@@ -290,7 +290,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 113]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mvp/modeling_mvp.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mvp/modeling_mvp.py
@@ -257,7 +257,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 114]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nat/modeling_nat.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nat/modeling_nat.py
@@ -343,7 +343,7 @@
         attention_scores = natten2dqkrpb(query_layer, key_layer, self.rpb, self.kernel_size, 1)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 115]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
@@ -321,7 +321,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 116]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nllb_moe/modeling_nllb_moe.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nllb_moe/modeling_nllb_moe.py
@@ -601,7 +601,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 117]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
@@ -194,7 +194,7 @@
                 # Apply the attention mask is (precomputed for all layers in NystromformerModel forward() function)
                 attention_scores = attention_scores + attention_mask
 
-            attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
             context_layer = torch.matmul(attention_probs, value_layer)
 
         else:
@@ -222,7 +222,7 @@
                 # Apply the attention mask is (precomputed for all layers in NystromformerModel forward() function)
                 attention_scores = attention_scores + attention_mask
 
-            kernel_3 = nn.functional.softmax(attention_scores, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; kernel_3 = softmax_softmax(attention_scores)
             attention_probs = torch.matmul(kernel_1, self.iterative_inv(kernel_2))
             new_value_layer = torch.matmul(kernel_3, value_layer)
             context_layer = torch.matmul(attention_probs, new_value_layer)
[RULE 4][MODEL 118]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/oneformer/modeling_oneformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/oneformer/modeling_oneformer.py
@@ -1598,7 +1598,7 @@
                 )
             attn_weights += attention_mask
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 119]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/openai/modeling_openai.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/openai/modeling_openai.py
@@ -183,7 +183,7 @@
             # Apply the attention mask
             w = w + attention_mask
 
-        w = nn.functional.softmax(w, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; w = softmax_softmax(w)
         w = self.attn_dropout(w)
 
         # Mask heads if we want to
[RULE 4][MODEL 120]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/opt/modeling_opt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/opt/modeling_opt.py
@@ -230,7 +230,7 @@
         if attn_weights.dtype == torch.float16:
             attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)
         else:
-            attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 121]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/owlv2/modeling_owlv2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/owlv2/modeling_owlv2.py
@@ -429,7 +429,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 122]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/owlvit/modeling_owlvit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/owlvit/modeling_owlvit.py
@@ -422,7 +422,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 123]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pegasus/modeling_pegasus.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pegasus/modeling_pegasus.py
@@ -252,7 +252,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 124]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pegasus_x/modeling_pegasus_x.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pegasus_x/modeling_pegasus_x.py
@@ -261,7 +261,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
@@ -467,7 +467,7 @@
         # [batch_size, num_heads, global_len, global_len+padded_seq_len]
         attn_weights = torch.einsum("BHGF,BHXF->BHGX", global_q, global_and_local_k)
         attn_weights = attn_weights + extended_mask[:, None, None, :]
-        attn_probs = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_probs = softmax_softmax(attn_weights)
         attn_probs = nn.functional.dropout(attn_probs, p=self.dropout, training=self.training)
 
         # [batch_size, num_heads, global_len, F]
@@ -521,7 +521,7 @@
         # [batch_size, num_heads, num_blocks, block_size, global_len+block_size]
         attn_weights = torch.cat([blocked_local2global, blocked_local2local], dim=-1)
         attn_weights = attn_weights + extended_mask[:, None, :, None, :]
-        attn_probs = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_probs = softmax_softmax(attn_weights)
         attn_probs = nn.functional.dropout(attn_probs, p=self.dropout, training=self.training)
 
         # [batch_size, num_heads, num_blocks, block_size, global_len]
[RULE 4][MODEL 125]
[RULE 4][MODEL 126]
[RULE 4][MODEL 127]
[RULE 4][MODEL 128]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/plbart/modeling_plbart.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/plbart/modeling_plbart.py
@@ -245,7 +245,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 129]
[RULE 4][MODEL 130]
[RULE 4][MODEL 131]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/prophetnet/modeling_prophetnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/prophetnet/modeling_prophetnet.py
@@ -725,7 +725,7 @@
         else:
             attn_weights_reshaped = None
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             assert layer_head_mask.size() == (self.num_attn_heads,), (
[RULE 4][MODEL 132]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pvt/modeling_pvt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pvt/modeling_pvt.py
@@ -234,7 +234,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 133]
[RULE 4][MODEL 134]
[RULE 4][MODEL 135]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
@@ -339,7 +339,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 136]
[RULE 4][MODEL 137]
[RULE 4][MODEL 138]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
@@ -293,7 +293,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 139]
[RULE 4][MODEL 140]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
@@ -263,7 +263,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 141]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
@@ -266,7 +266,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 142]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
@@ -397,7 +397,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 143]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
@@ -293,7 +293,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 144]
[RULE 4][MODEL 145]
[RULE 4][MODEL 146]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/segformer/modeling_segformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/segformer/modeling_segformer.py
@@ -209,7 +209,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 147]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
@@ -500,7 +500,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 148]
[RULE 4][MODEL 149]
[RULE 4][MODEL 150]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speech_to_text/modeling_speech_to_text.py
@@ -312,7 +312,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 151]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py
@@ -258,7 +258,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 152]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speecht5/modeling_speecht5.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speecht5/modeling_speecht5.py
@@ -1004,7 +1004,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 153]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
@@ -212,7 +212,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 154]
[RULE 4][MODEL 155]
[RULE 4][MODEL 156]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swin/modeling_swin.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swin/modeling_swin.py
@@ -495,7 +495,7 @@
             attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 157]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swin2sr/modeling_swin2sr.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swin2sr/modeling_swin2sr.py
@@ -375,7 +375,7 @@
             attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 158]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swinv2/modeling_swinv2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/swinv2/modeling_swinv2.py
@@ -529,7 +529,7 @@
             attention_scores = attention_scores.view(-1, self.num_attention_heads, dim, dim)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 159]
[RULE 4][MODEL 160]
[RULE 4][MODEL 161]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/table_transformer/modeling_table_transformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/table_transformer/modeling_table_transformer.py
@@ -579,7 +579,7 @@
             attn_weights = attn_weights.view(batch_size, self.num_heads, target_len, source_len) + attention_mask
             attn_weights = attn_weights.view(batch_size * self.num_heads, target_len, source_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit awkward, but it's required to
[RULE 4][MODEL 162]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
@@ -410,7 +410,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 163]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/time_series_transformer/modeling_time_series_transformer.py
@@ -414,7 +414,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 164]
[RULE 4][MODEL 165]
[RULE 4][MODEL 166]
[RULE 4][MODEL 167]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/trocr/modeling_trocr.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/trocr/modeling_trocr.py
@@ -269,7 +269,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 168]
[RULE 4][MODEL 169]
[RULE 4][MODEL 170]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
@@ -540,7 +540,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 171]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
@@ -554,7 +554,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 172]
[RULE 4][MODEL 173]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/videomae/modeling_videomae.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/videomae/modeling_videomae.py
@@ -252,7 +252,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 174]
[RULE 4][MODEL 175]
[RULE 4][MODEL 176]
[RULE 4][MODEL 177]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
@@ -243,7 +243,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 178]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit/modeling_vit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit/modeling_vit.py
@@ -221,7 +221,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 179]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py
@@ -233,7 +233,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 180]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_mae/modeling_vit_mae.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_mae/modeling_vit_mae.py
@@ -354,7 +354,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 181]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_msn/modeling_vit_msn.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_msn/modeling_vit_msn.py
@@ -205,7 +205,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 182]
[RULE 4][MODEL 183]
[RULE 4][MODEL 184]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vits/modeling_vits.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vits/modeling_vits.py
@@ -268,7 +268,7 @@
     if min_bin_height * num_bins > 1.0:
         raise ValueError(f"Minimal bin height {min_bin_height} too large for the number of bins {num_bins}")
 
-    widths = nn.functional.softmax(unnormalized_widths, dim=-1)
+    from xformers.triton.softmax import softmax as softmax_softmax; widths = softmax_softmax(unnormalized_widths)
     widths = min_bin_width + (1 - min_bin_width * num_bins) * widths
     cumwidths = torch.cumsum(widths, dim=-1)
     cumwidths = nn.functional.pad(cumwidths, pad=(1, 0), mode="constant", value=0.0)
@@ -279,7 +279,7 @@
 
     derivatives = min_derivative + nn.functional.softplus(unnormalized_derivatives)
 
-    heights = nn.functional.softmax(unnormalized_heights, dim=-1)
+    from xformers.triton.softmax import softmax as softmax_softmax; heights = softmax_softmax(unnormalized_heights)
     heights = min_bin_height + (1 - min_bin_height * num_bins) * heights
     cumheights = torch.cumsum(heights, dim=-1)
     cumheights = nn.functional.pad(cumheights, pad=(1, 0), mode="constant", value=0.0)
@@ -961,7 +961,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 185]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vivit/modeling_vivit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vivit/modeling_vivit.py
@@ -160,7 +160,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 186]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
@@ -606,7 +606,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 187]
[RULE 4][MODEL 188]
[RULE 4][MODEL 189]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/whisper/modeling_whisper.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/whisper/modeling_whisper.py
@@ -433,7 +433,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 190]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/x_clip/modeling_x_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/x_clip/modeling_x_clip.py
@@ -261,7 +261,7 @@
             attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
             attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if output_attentions:
             # this operation is a bit akward, but it's required to
[RULE 4][MODEL 191]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xglm/modeling_xglm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xglm/modeling_xglm.py
@@ -323,7 +323,7 @@
         if attn_weights.dtype == torch.float16:
             attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(torch.float16)
         else:
-            attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+            from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             if layer_head_mask.size() != (self.num_heads,):
[RULE 4][MODEL 192]
[RULE 4][MODEL 193]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_prophetnet/modeling_xlm_prophetnet.py
@@ -740,7 +740,7 @@
         else:
             attn_weights_reshaped = None
 
-        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attn_weights = softmax_softmax(attn_weights)
 
         if layer_head_mask is not None:
             assert layer_head_mask.size() == (self.num_attn_heads,), (
[RULE 4][MODEL 194]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
@@ -264,7 +264,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 195]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
@@ -257,7 +257,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 196]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlnet/modeling_xlnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlnet/modeling_xlnet.py
@@ -2041,7 +2041,7 @@
         else:
             # during inference, compute the end logits based on beam search
             bsz, slen, hsz = hidden_states.size()
-            start_log_probs = nn.functional.softmax(start_logits, dim=-1)  # shape (bsz, slen)
+            from xformers.triton.softmax import softmax as softmax_softmax; start_log_probs = softmax_softmax(start_logits)  # shape (bsz, slen)
 
             start_top_log_probs, start_top_index = torch.topk(
                 start_log_probs, self.start_n_top, dim=-1
[RULE 4][MODEL 197]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
@@ -257,7 +257,7 @@
             attention_scores = attention_scores + attention_mask
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 198]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yolos/modeling_yolos.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yolos/modeling_yolos.py
@@ -288,7 +288,7 @@
         attention_scores = attention_scores / math.sqrt(self.attention_head_size)
 
         # Normalize the attention scores to probabilities.
-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)
+        from xformers.triton.softmax import softmax as softmax_softmax; attention_probs = softmax_softmax(attention_scores)
 
         # This is actually dropping out entire tokens to attend to, which might
         # seem a bit unusual, but is taken from the original Transformer paper.
[RULE 4][MODEL 199]
[RULE 5_1]
[RULE 5_1][MODEL 1]
[RULE 5_1][MODEL 2]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
@@ -881,8 +881,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -962,8 +961,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 3]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
@@ -432,8 +432,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -513,8 +512,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 4]
[RULE 5_1][MODEL 5]
[RULE 5_1][MODEL 6]
[RULE 5_1][MODEL 7]
[RULE 5_1][MODEL 8]
[RULE 5_1][MODEL 9]
[RULE 5_1][MODEL 10]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
@@ -384,8 +384,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -462,8 +461,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 11]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
@@ -52,8 +52,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -268,8 +267,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 12]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
@@ -1318,8 +1318,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -1437,8 +1436,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 13]
[RULE 5_1][MODEL 14]
[RULE 5_1][MODEL 15]
[RULE 5_1][MODEL 16]
[RULE 5_1][MODEL 17]
[RULE 5_1][MODEL 18]
[RULE 5_1][MODEL 19]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
@@ -716,8 +716,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -796,8 +795,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 20]
[RULE 5_1][MODEL 21]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
@@ -378,8 +378,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -409,8 +408,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 22]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
@@ -433,8 +433,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -517,8 +516,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 23]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
@@ -310,8 +310,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -391,8 +390,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 24]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
@@ -510,8 +510,7 @@
         self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor
     ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -682,8 +681,7 @@
 
     def forward(self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor) -> torch.FloatTensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 25]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
@@ -351,8 +351,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -516,8 +515,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 26]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
@@ -1381,8 +1381,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -1462,8 +1461,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 27]
[RULE 5_1][MODEL 28]
[RULE 5_1][MODEL 29]
[RULE 5_1][MODEL 30]
[RULE 5_1][MODEL 31]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
@@ -432,8 +432,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -538,8 +537,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 32]
[RULE 5_1][MODEL 33]
[RULE 5_1][MODEL 34]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
@@ -523,8 +523,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 35]
[RULE 5_1][MODEL 36]
[RULE 5_1][MODEL 37]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta/modeling_deberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta/modeling_deberta.py
@@ -292,8 +292,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -359,8 +358,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 38]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta_v2/modeling_deberta_v2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta_v2/modeling_deberta_v2.py
@@ -268,8 +268,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -337,8 +336,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 39]
[RULE 5_1][MODEL 40]
[RULE 5_1][MODEL 41]
[RULE 5_1][MODEL 42]
[RULE 5_1][MODEL 43]
[RULE 5_1][MODEL 44]
[RULE 5_1][MODEL 45]
[RULE 5_1][MODEL 46]
[RULE 5_1][MODEL 47]
[RULE 5_1][MODEL 48]
[RULE 5_1][MODEL 49]
[RULE 5_1][MODEL 50]
[RULE 5_1][MODEL 51]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
@@ -357,8 +357,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -438,8 +437,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 52]
[RULE 5_1][MODEL 53]
[RULE 5_1][MODEL 54]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
@@ -292,8 +292,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -373,8 +372,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 55]
[RULE 5_1][MODEL 56]
[RULE 5_1][MODEL 57]
[RULE 5_1][MODEL 58]
[RULE 5_1][MODEL 59]
[RULE 5_1][MODEL 60]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
@@ -245,8 +245,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 61]
[RULE 5_1][MODEL 62]
[RULE 5_1][MODEL 63]
[RULE 5_1][MODEL 64]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
@@ -278,8 +278,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -358,8 +357,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 65]
[RULE 5_1][MODEL 66]
[RULE 5_1][MODEL 67]
[RULE 5_1][MODEL 68]
[RULE 5_1][MODEL 69]
[RULE 5_1][MODEL 70]
[RULE 5_1][MODEL 71]
[RULE 5_1][MODEL 72]
[RULE 5_1][MODEL 73]
[RULE 5_1][MODEL 74]
[RULE 5_1][MODEL 75]
[RULE 5_1][MODEL 76]
[RULE 5_1][MODEL 77]
[RULE 5_1][MODEL 78]
[RULE 5_1][MODEL 79]
[RULE 5_1][MODEL 80]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
@@ -706,8 +706,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -787,8 +786,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 81]
[RULE 5_1][MODEL 82]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
@@ -273,8 +273,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -354,8 +353,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 83]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
@@ -237,8 +237,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -268,8 +267,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 84]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
@@ -475,8 +475,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -737,8 +736,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 85]
[RULE 5_1][MODEL 86]
[RULE 5_1][MODEL 87]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
@@ -338,8 +338,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -421,8 +420,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 88]
[RULE 5_1][MODEL 89]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
@@ -1139,8 +1139,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -1219,8 +1218,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 90]
[RULE 5_1][MODEL 91]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
@@ -625,8 +625,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -703,8 +702,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 92]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lxmert/modeling_lxmert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lxmert/modeling_lxmert.py
@@ -379,8 +379,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -441,8 +440,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 93]
[RULE 5_1][MODEL 94]
[RULE 5_1][MODEL 95]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
@@ -233,8 +233,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -264,8 +263,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 96]
[RULE 5_1][MODEL 97]
[RULE 5_1][MODEL 98]
[RULE 5_1][MODEL 99]
[RULE 5_1][MODEL 100]
[RULE 5_1][MODEL 101]
[RULE 5_1][MODEL 102]
[RULE 5_1][MODEL 103]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilebert/modeling_mobilebert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mobilebert/modeling_mobilebert.py
@@ -316,8 +316,7 @@
     def forward(self, hidden_states: torch.Tensor, residual_tensor: torch.Tensor) -> torch.Tensor:
         layer_outputs = self.dense(hidden_states)
         if not self.use_bottleneck:
-            layer_outputs = self.dropout(layer_outputs)
-        layer_outputs = self.LayerNorm(layer_outputs + residual_tensor)
+            self.fused_dropout_layernorm(layer_outputs + residual_tensor)
         return layer_outputs
 
 
@@ -395,8 +394,7 @@
 
     def forward(self, hidden_states: torch.Tensor, residual_tensor: torch.Tensor) -> torch.Tensor:
         layer_outputs = self.dense(hidden_states)
-        layer_outputs = self.dropout(layer_outputs)
-        layer_outputs = self.LayerNorm(layer_outputs + residual_tensor)
+        self.fused_dropout_layernorm(layer_outputs + residual_tensor)
         return layer_outputs
 
 
@@ -416,8 +414,7 @@
     ) -> torch.Tensor:
         layer_output = self.dense(intermediate_states)
         if not self.use_bottleneck:
-            layer_output = self.dropout(layer_output)
-            layer_output = self.LayerNorm(layer_output + residual_tensor_1)
+            self.fused_dropout_layernorm(layer_output + residual_tensor_1)
         else:
             layer_output = self.LayerNorm(layer_output + residual_tensor_1)
             layer_output = self.bottleneck(layer_output, residual_tensor_2)
[RULE 5_1][MODEL 104]
[RULE 5_1][MODEL 105]
[RULE 5_1][MODEL 106]
[RULE 5_1][MODEL 107]
[RULE 5_1][MODEL 108]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
@@ -276,8 +276,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 109]
[RULE 5_1][MODEL 110]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
@@ -647,8 +647,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -710,8 +709,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 111]
[RULE 5_1][MODEL 112]
[RULE 5_1][MODEL 113]
[RULE 5_1][MODEL 114]
[RULE 5_1][MODEL 115]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
@@ -365,8 +365,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -445,8 +444,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 116]
[RULE 5_1][MODEL 117]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
@@ -249,8 +249,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -312,8 +311,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 118]
[RULE 5_1][MODEL 119]
[RULE 5_1][MODEL 120]
[RULE 5_1][MODEL 121]
[RULE 5_1][MODEL 122]
[RULE 5_1][MODEL 123]
[RULE 5_1][MODEL 124]
[RULE 5_1][MODEL 125]
[RULE 5_1][MODEL 126]
[RULE 5_1][MODEL 127]
[RULE 5_1][MODEL 128]
[RULE 5_1][MODEL 129]
[RULE 5_1][MODEL 130]
[RULE 5_1][MODEL 131]
[RULE 5_1][MODEL 132]
[RULE 5_1][MODEL 133]
[RULE 5_1][MODEL 134]
[RULE 5_1][MODEL 135]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
@@ -372,8 +372,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -453,8 +452,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 136]
[RULE 5_1][MODEL 137]
[RULE 5_1][MODEL 138]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
@@ -326,8 +326,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -408,8 +407,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 139]
[RULE 5_1][MODEL 140]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
@@ -296,8 +296,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -377,8 +376,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 141]
[RULE 5_1][MODEL 142]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
@@ -430,8 +430,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -511,8 +510,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 143]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
@@ -353,8 +353,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -437,8 +436,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 144]
[RULE 5_1][MODEL 145]
[RULE 5_1][MODEL 146]
[RULE 5_1][MODEL 147]
[RULE 5_1][MODEL 148]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew_d/modeling_sew_d.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew_d/modeling_sew_d.py
@@ -677,8 +677,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -968,8 +967,7 @@
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 149]
[RULE 5_1][MODEL 150]
[RULE 5_1][MODEL 151]
[RULE 5_1][MODEL 152]
[RULE 5_1][MODEL 153]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
@@ -245,8 +245,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -326,8 +325,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 154]
[RULE 5_1][MODEL 155]
[RULE 5_1][MODEL 156]
[RULE 5_1][MODEL 157]
[RULE 5_1][MODEL 158]
[RULE 5_1][MODEL 159]
[RULE 5_1][MODEL 160]
[RULE 5_1][MODEL 161]
[RULE 5_1][MODEL 162]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
@@ -442,8 +442,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -524,8 +523,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 163]
[RULE 5_1][MODEL 164]
[RULE 5_1][MODEL 165]
[RULE 5_1][MODEL 166]
[RULE 5_1][MODEL 167]
[RULE 5_1][MODEL 168]
[RULE 5_1][MODEL 169]
[RULE 5_1][MODEL 170]
[RULE 5_1][MODEL 171]
[RULE 5_1][MODEL 172]
[RULE 5_1][MODEL 173]
[RULE 5_1][MODEL 174]
[RULE 5_1][MODEL 175]
[RULE 5_1][MODEL 176]
[RULE 5_1][MODEL 177]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
@@ -274,8 +274,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -348,8 +347,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 178]
[RULE 5_1][MODEL 179]
[RULE 5_1][MODEL 180]
[RULE 5_1][MODEL 181]
[RULE 5_1][MODEL 182]
[RULE 5_1][MODEL 183]
[RULE 5_1][MODEL 184]
[RULE 5_1][MODEL 185]
[RULE 5_1][MODEL 186]
[RULE 5_1][MODEL 187]
[RULE 5_1][MODEL 188]
[RULE 5_1][MODEL 189]
[RULE 5_1][MODEL 190]
[RULE 5_1][MODEL 191]
[RULE 5_1][MODEL 192]
[RULE 5_1][MODEL 193]
[RULE 5_1][MODEL 194]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
@@ -297,8 +297,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -378,8 +377,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_1][MODEL 195]
[RULE 5_1][MODEL 196]
[RULE 5_1][MODEL 197]
[RULE 5_1][MODEL 198]
[RULE 5_1][MODEL 199]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
@@ -440,8 +440,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
@@ -503,8 +502,7 @@
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
-        hidden_states = self.dropout(hidden_states)
-        hidden_states = self.LayerNorm(hidden_states + input_tensor)
+        self.fused_dropout_layernorm(hidden_states + input_tensor)
         return hidden_states
 
 
[RULE 5_2]
[RULE 5_2][MODEL 1]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/albert/modeling_albert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/albert/modeling_albert.py
@@ -204,8 +204,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.embedding_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
[RULE 5_2][MODEL 2]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
@@ -683,8 +683,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -876,8 +875,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -957,8 +955,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 3]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
@@ -212,8 +212,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -427,8 +426,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -508,8 +506,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 4]
[RULE 5_2][MODEL 5]
[RULE 5_2][MODEL 6]
[RULE 5_2][MODEL 7]
[RULE 5_2][MODEL 8]
[RULE 5_2][MODEL 9]
[RULE 5_2][MODEL 10]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
@@ -188,8 +188,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -379,8 +378,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -457,8 +455,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 11]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
@@ -47,8 +47,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -263,8 +262,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -552,8 +550,7 @@
         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
[RULE 5_2][MODEL 12]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
@@ -253,8 +253,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -1313,8 +1312,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -1432,8 +1430,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 13]
[RULE 5_2][MODEL 14]
[RULE 5_2][MODEL 15]
[RULE 5_2][MODEL 16]
[RULE 5_2][MODEL 17]
[RULE 5_2][MODEL 18]
[RULE 5_2][MODEL 19]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
@@ -711,8 +711,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -791,8 +790,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -1012,8 +1010,7 @@
         super().__init__(config)
         self.config = config
 
-        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.encoder = Blip2QFormerEncoder(config)
 
[RULE 5_2][MODEL 20]
[RULE 5_2][MODEL 21]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
@@ -373,8 +373,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -404,8 +403,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -877,8 +875,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
[RULE 5_2][MODEL 22]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
@@ -231,8 +231,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)))
@@ -428,8 +427,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -512,8 +510,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 23]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
@@ -90,8 +90,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -305,8 +304,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -386,8 +384,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 24]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
@@ -212,8 +212,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -359,8 +358,7 @@
         self.activation = ACT2FN[config.hidden_act]
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(
         self,
@@ -503,8 +501,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(
         self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor
@@ -677,8 +674,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: Tuple[torch.FloatTensor], input_tensor: torch.FloatTensor) -> torch.FloatTensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 25]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
@@ -117,8 +117,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -346,8 +345,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -511,8 +509,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 26]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
@@ -1161,8 +1161,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -1376,8 +1375,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -1457,8 +1455,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 27]
[RULE 5_2][MODEL 28]
[RULE 5_2][MODEL 29]
[RULE 5_2][MODEL 30]
[RULE 5_2][MODEL 31]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/convbert/modeling_convbert.py
@@ -188,8 +188,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.embedding_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
             "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False
@@ -427,8 +426,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -533,8 +531,7 @@
             self.dense = GroupedLinearLayer(
                 input_size=config.intermediate_size, output_size=config.hidden_size, num_groups=config.num_groups
             )
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 32]
[RULE 5_2][MODEL 33]
[RULE 5_2][MODEL 34]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
@@ -518,8 +518,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 35]
[RULE 5_2][MODEL 36]
[RULE 5_2][MODEL 37]
[RULE 5_2][MODEL 38]
[RULE 5_2][MODEL 39]
[RULE 5_2][MODEL 40]
[RULE 5_2][MODEL 41]
[RULE 5_2][MODEL 42]
[RULE 5_2][MODEL 43]
[RULE 5_2][MODEL 44]
[RULE 5_2][MODEL 45]
[RULE 5_2][MODEL 46]
[RULE 5_2][MODEL 47]
[RULE 5_2][MODEL 48]
[RULE 5_2][MODEL 49]
[RULE 5_2][MODEL 50]
[RULE 5_2][MODEL 51]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
@@ -157,8 +157,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.embedding_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -352,8 +351,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -433,8 +431,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 52]
[RULE 5_2][MODEL 53]
[RULE 5_2][MODEL 54]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
@@ -85,8 +85,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -287,8 +286,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -368,8 +366,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 55]
[RULE 5_2][MODEL 56]
[RULE 5_2][MODEL 57]
[RULE 5_2][MODEL 58]
[RULE 5_2][MODEL 59]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
@@ -383,8 +383,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
[RULE 5_2][MODEL 60]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
@@ -240,8 +240,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 61]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/focalnet/modeling_focalnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/focalnet/modeling_focalnet.py
@@ -194,8 +194,7 @@
         self.patch_grid = self.patch_embeddings.grid_size
         self.mask_token = nn.Parameter(torch.zeros(1, 1, config.embed_dim)) if use_mask_token else None
 
-        self.norm = nn.LayerNorm(config.embed_dim, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.embed_dim, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(
         self, pixel_values: Optional[torch.FloatTensor], bool_masked_pos: Optional[torch.BoolTensor] = None
[RULE 5_2][MODEL 62]
[RULE 5_2][MODEL 63]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/funnel/modeling_funnel.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/funnel/modeling_funnel.py
@@ -159,8 +159,7 @@
     def __init__(self, config: FunnelConfig) -> None:
         super().__init__()
         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
-        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.d_model, config.hidden_dropout, config.layer_norm_eps)
 
     def forward(
         self, input_ids: Optional[torch.Tensor] = None, inputs_embeds: Optional[torch.Tensor] = None
[RULE 5_2][MODEL 64]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
@@ -105,8 +105,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -273,8 +272,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -353,8 +351,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 65]
[RULE 5_2][MODEL 66]
[RULE 5_2][MODEL 67]
[RULE 5_2][MODEL 68]
[RULE 5_2][MODEL 69]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neox/modeling_gpt_neox.py
@@ -405,8 +405,7 @@
         super().__init__()
         self.use_parallel_residual = config.use_parallel_residual
         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.post_attention_dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.post_mlp_dropout = nn.Dropout(config.hidden_dropout)
         self.attention = GPTNeoXAttention(config)
         self.mlp = GPTNeoXMLP(config)
[RULE 5_2][MODEL 70]
[RULE 5_2][MODEL 71]
[RULE 5_2][MODEL 72]
[RULE 5_2][MODEL 73]
[RULE 5_2][MODEL 74]
[RULE 5_2][MODEL 75]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
@@ -685,8 +685,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = HubertPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([HubertEncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.gradient_checkpointing = False
 
@@ -773,8 +772,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = HubertPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [HubertEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]
         )
[RULE 5_2][MODEL 76]
[RULE 5_2][MODEL 77]
[RULE 5_2][MODEL 78]
[RULE 5_2][MODEL 79]
[RULE 5_2][MODEL 80]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
@@ -701,8 +701,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -782,8 +781,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -1006,8 +1004,7 @@
         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
 
-        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
[RULE 5_2][MODEL 81]
[RULE 5_2][MODEL 82]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
@@ -268,8 +268,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -349,8 +348,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 83]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
@@ -74,8 +74,7 @@
         self.w_position_embeddings = nn.Embedding(config.max_2d_position_embeddings, config.shape_size)
         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.register_buffer(
             "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False
@@ -232,8 +231,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states, input_tensor):
         hidden_states = self.dense(hidden_states)
@@ -263,8 +261,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -711,8 +708,7 @@
         self.visual_proj = nn.Linear(config.image_feature_pool_shape[-1], config.hidden_size)
         if self.has_visual_segment_embedding:
             self.visual_segment_embedding = nn.Parameter(nn.Embedding(1, config.hidden_size).weight[0])
-        self.visual_LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.visual_dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.encoder = LayoutLMv2Encoder(config)
         self.pooler = LayoutLMv2Pooler(config)
[RULE 5_2][MODEL 84]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
@@ -241,8 +241,7 @@
         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)
         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -470,8 +469,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -732,8 +730,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -764,8 +761,7 @@
             self.pos_embed = nn.Parameter(torch.zeros(1, size * size + 1, config.hidden_size))
             self.pos_drop = nn.Dropout(p=0.0)
 
-            self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-            self.dropout = nn.Dropout(config.hidden_dropout_prob)
+            from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
             if self.config.has_relative_attention_bias or self.config.has_spatial_attention_bias:
                 self.init_visual_bbox(image_size=(size, size))
[RULE 5_2][MODEL 85]
[RULE 5_2][MODEL 86]
[RULE 5_2][MODEL 87]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
@@ -55,8 +55,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -333,8 +332,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -416,8 +414,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 88]
[RULE 5_2][MODEL 89]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
@@ -442,8 +442,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.padding_idx = config.pad_token_id
         self.position_embeddings = nn.Embedding(
@@ -1134,8 +1133,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -1214,8 +1212,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 90]
[RULE 5_2][MODEL 91]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
@@ -402,8 +402,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # End copy
         self.padding_idx = config.pad_token_id
@@ -474,8 +473,7 @@
         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)
         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(
         self, entity_ids: torch.LongTensor, position_ids: torch.LongTensor, token_type_ids: torch.LongTensor = None
@@ -620,8 +618,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -698,8 +695,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 92]
[RULE 5_2][MODEL 93]
[RULE 5_2][MODEL 94]
[RULE 5_2][MODEL 95]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
@@ -140,8 +140,7 @@
 
         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.register_buffer(
             "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False
@@ -228,8 +227,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -259,8 +257,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 96]
[RULE 5_2][MODEL 97]
[RULE 5_2][MODEL 98]
[RULE 5_2][MODEL 99]
[RULE 5_2][MODEL 100]
[RULE 5_2][MODEL 101]
[RULE 5_2][MODEL 102]
[RULE 5_2][MODEL 103]
[RULE 5_2][MODEL 104]
[RULE 5_2][MODEL 105]
[RULE 5_2][MODEL 106]
[RULE 5_2][MODEL 107]
[RULE 5_2][MODEL 108]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
@@ -81,8 +81,7 @@
             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx
         )
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         self.register_buffer(
             "position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False
         )
@@ -208,8 +207,7 @@
     def __init__(self, config):
         super().__init__()
         self.attn = MPNetSelfAttention(config)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.pruned_heads = set()
 
@@ -271,8 +269,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 109]
[RULE 5_2][MODEL 110]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
@@ -505,8 +505,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer("position_ids", torch.arange(config.max_position_embeddings).expand((1, -1)) + 2)
@@ -642,8 +641,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -705,8 +703,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 111]
[RULE 5_2][MODEL 112]
[RULE 5_2][MODEL 113]
[RULE 5_2][MODEL 114]
[RULE 5_2][MODEL 115]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
@@ -179,8 +179,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         self.register_buffer(
             "token_type_ids", torch.zeros((1, config.max_position_embeddings), dtype=torch.long), persistent=False
         )
@@ -360,8 +359,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -440,8 +438,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 116]
[RULE 5_2][MODEL 117]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
@@ -60,8 +60,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -244,8 +243,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -307,8 +305,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 118]
[RULE 5_2][MODEL 119]
[RULE 5_2][MODEL 120]
[RULE 5_2][MODEL 121]
[RULE 5_2][MODEL 122]
[RULE 5_2][MODEL 123]
[RULE 5_2][MODEL 124]
[RULE 5_2][MODEL 125]
[RULE 5_2][MODEL 126]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/persimmon/modeling_persimmon.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/persimmon/modeling_persimmon.py
@@ -362,8 +362,7 @@
         self.self_attn = PersimmonAttention(config=config)
         self.mlp = PersimmonMLP(config)
         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
 
     def forward(
         self,
[RULE 5_2][MODEL 127]
[RULE 5_2][MODEL 128]
[RULE 5_2][MODEL 129]
[RULE 5_2][MODEL 130]
[RULE 5_2][MODEL 131]
[RULE 5_2][MODEL 132]
[RULE 5_2][MODEL 133]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/qdqbert/modeling_qdqbert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/qdqbert/modeling_qdqbert.py
@@ -160,8 +160,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -353,8 +352,7 @@
         # Quantize Linear layer
         self.dense = quant_nn.QuantLinear(config.hidden_size, config.hidden_size)
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # Quantize the inputs to the residual add
         self.add_local_input_quantizer = TensorQuantizer(quant_nn.QuantLinear.default_quant_desc_input)
@@ -441,8 +439,7 @@
         super().__init__()
         # Quantize Linear layer
         self.dense = quant_nn.QuantLinear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # Quantize the inputs to the residual add
         self.add_local_input_quantizer = TensorQuantizer(quant_nn.QuantLinear.default_quant_desc_input)
[RULE 5_2][MODEL 134]
[RULE 5_2][MODEL 135]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
@@ -174,8 +174,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -367,8 +366,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -448,8 +446,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 136]
[RULE 5_2][MODEL 137]
[RULE 5_2][MODEL 138]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
@@ -154,8 +154,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.input_embedding_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.input_embedding_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -321,8 +320,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -403,8 +401,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 139]
[RULE 5_2][MODEL 140]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
@@ -76,8 +76,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -291,8 +290,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -372,8 +370,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 141]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
@@ -79,8 +79,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
[RULE 5_2][MODEL 142]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
@@ -186,8 +186,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -425,8 +424,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -506,8 +504,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 143]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
@@ -181,8 +181,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.embedding_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.embedding_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, input_ids=None, token_type_ids=None, inputs_embeds=None):
         if input_ids is not None:
@@ -348,8 +347,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -432,8 +430,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 144]
[RULE 5_2][MODEL 145]
[RULE 5_2][MODEL 146]
[RULE 5_2][MODEL 147]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
@@ -609,8 +609,7 @@
         self.config = config
         self.pos_conv_embed = SEWPositionalConvEmbedding(config)
         self.pool = nn.AvgPool1d(config.squeeze_factor, config.squeeze_factor)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([SEWEncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.upsample = SEWUpsampling(config)
         self.gradient_checkpointing = False
[RULE 5_2][MODEL 148]
[RULE 5_2][MODEL 149]
[RULE 5_2][MODEL 150]
[RULE 5_2][MODEL 151]
[RULE 5_2][MODEL 152]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speecht5/modeling_speecht5.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/speecht5/modeling_speecht5.py
@@ -1293,8 +1293,7 @@
 
     def __init__(self, config: SpeechT5Config):
         super().__init__(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layerdrop = config.encoder_layerdrop
 
         self.layers = nn.ModuleList([SpeechT5EncoderLayer(config) for _ in range(config.encoder_layers)])
[RULE 5_2][MODEL 153]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
@@ -57,8 +57,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -240,8 +239,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -321,8 +319,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 154]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/squeezebert/modeling_squeezebert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/squeezebert/modeling_squeezebert.py
@@ -60,8 +60,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
[RULE 5_2][MODEL 155]
[RULE 5_2][MODEL 156]
[RULE 5_2][MODEL 157]
[RULE 5_2][MODEL 158]
[RULE 5_2][MODEL 159]
[RULE 5_2][MODEL 160]
[RULE 5_2][MODEL 161]
[RULE 5_2][MODEL 162]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
@@ -280,8 +280,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         self.config = config
 
@@ -437,8 +436,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -519,8 +517,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 163]
[RULE 5_2][MODEL 164]
[RULE 5_2][MODEL 165]
[RULE 5_2][MODEL 166]
[RULE 5_2][MODEL 167]
[RULE 5_2][MODEL 168]
[RULE 5_2][MODEL 169]
[RULE 5_2][MODEL 170]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
@@ -721,8 +721,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = UniSpeechPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([UniSpeechEncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.gradient_checkpointing = False
 
@@ -809,8 +808,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = UniSpeechPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [UniSpeechEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]
         )
[RULE 5_2][MODEL 171]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
@@ -735,8 +735,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = UniSpeechSatPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([UniSpeechSatEncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.gradient_checkpointing = False
 
@@ -823,8 +822,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = UniSpeechSatPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [UniSpeechSatEncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]
         )
[RULE 5_2][MODEL 172]
[RULE 5_2][MODEL 173]
[RULE 5_2][MODEL 174]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vilt/modeling_vilt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vilt/modeling_vilt.py
@@ -245,8 +245,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
[RULE 5_2][MODEL 175]
[RULE 5_2][MODEL 176]
[RULE 5_2][MODEL 177]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
@@ -74,8 +74,7 @@
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
 
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -269,8 +268,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -343,8 +341,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 178]
[RULE 5_2][MODEL 179]
[RULE 5_2][MODEL 180]
[RULE 5_2][MODEL 181]
[RULE 5_2][MODEL 182]
[RULE 5_2][MODEL 183]
[RULE 5_2][MODEL 184]
[RULE 5_2][MODEL 185]
[RULE 5_2][MODEL 186]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
@@ -757,8 +757,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = Wav2Vec2PositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([Wav2Vec2EncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.gradient_checkpointing = False
 
@@ -844,8 +843,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = Wav2Vec2PositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [Wav2Vec2EncoderLayerStableLayerNorm(config) for _ in range(config.num_hidden_layers)]
         )
[RULE 5_2][MODEL 187]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py
@@ -864,8 +864,7 @@
             self.embed_positions = None
 
         self.pos_conv_embed = Wav2Vec2ConformerPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList([Wav2Vec2ConformerEncoderLayer(config) for _ in range(config.num_hidden_layers)])
         self.gradient_checkpointing = False
 
[RULE 5_2][MODEL 188]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py
@@ -672,8 +672,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = WavLMPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [WavLMEncoderLayer(config, has_relative_position_bias=(i == 0)) for i in range(config.num_hidden_layers)]
         )
@@ -760,8 +759,7 @@
         super().__init__()
         self.config = config
         self.pos_conv_embed = WavLMPositionalConvEmbedding(config)
-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout, config.layer_norm_eps)
         self.layers = nn.ModuleList(
             [
                 WavLMEncoderLayerStableLayerNorm(config, has_relative_position_bias=(i == 0))
[RULE 5_2][MODEL 189]
[RULE 5_2][MODEL 190]
[RULE 5_2][MODEL 191]
[RULE 5_2][MODEL 192]
[RULE 5_2][MODEL 193]
[RULE 5_2][MODEL 194]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
@@ -77,8 +77,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -292,8 +291,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -373,8 +371,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 195]
[RULE 5_2][MODEL 196]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlnet/modeling_xlnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlnet/modeling_xlnet.py
@@ -226,8 +226,7 @@
         self.r_w_bias = nn.Parameter(torch.FloatTensor(self.n_head, self.d_head))
         self.seg_embed = nn.Parameter(torch.FloatTensor(2, self.n_head, self.d_head))
 
-        self.layer_norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.dropout)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.d_model, config.dropout, config.layer_norm_eps)
 
     def prune_heads(self, heads):
         raise NotImplementedError
[RULE 5_2][MODEL 197]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
@@ -70,8 +70,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.position_embedding_type = getattr(config, "position_embedding_type", "absolute")
         self.register_buffer(
@@ -285,8 +284,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 5_2][MODEL 198]
[RULE 5_2][MODEL 199]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
@@ -248,8 +248,7 @@
 
         # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load
         # any TensorFlow checkpoint file
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
         # position_ids (1, len position emb) is contiguous in memory and exported when serialized
         self.register_buffer(
@@ -435,8 +434,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.hidden_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
@@ -498,8 +496,7 @@
     def __init__(self, config):
         super().__init__()
         self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dropout = nn.Dropout(config.hidden_dropout_prob)
+        from epoi.ops.torchscript_ops import FusedDropoutAddLayerNorm; self.fused_dropout_layernorm = epoi.ops.torchscript_ops.FusedDropoutAddLayerNorm(config.hidden_size, config.hidden_dropout_prob, config.layer_norm_eps)
 
     def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6]
[RULE 6][MODEL 1]
[RULE 6][MODEL 2]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/align/modeling_align.py
@@ -940,11 +940,7 @@
 class AlignTextIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 3]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/altclip/modeling_altclip.py
@@ -491,11 +491,7 @@
 class AltRobertaIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 4]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py
@@ -238,11 +238,7 @@
 class ASTIntermediate(nn.Module):
     def __init__(self, config: ASTConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 5]
[RULE 6][MODEL 6]
[RULE 6][MODEL 7]
[RULE 6][MODEL 8]
[RULE 6][MODEL 9]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/beit/modeling_beit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/beit/modeling_beit.py
@@ -344,11 +344,7 @@
 class BeitIntermediate(nn.Module):
     def __init__(self, config: BeitConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 10]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert/modeling_bert.py
@@ -441,11 +441,7 @@
 class BertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 11]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bert_generation/modeling_bert_generation.py
@@ -246,11 +246,7 @@
 class BertGenerationIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 12]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/big_bird/modeling_big_bird.py
@@ -1415,11 +1415,7 @@
 class BigBirdIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 13]
[RULE 6][MODEL 14]
[RULE 6][MODEL 15]
[RULE 6][MODEL 16]
[RULE 6][MODEL 17]
[RULE 6][MODEL 18]
[RULE 6][MODEL 19]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/blip_2/modeling_blip_2.py
@@ -774,11 +774,7 @@
 class Blip2QFormerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 20]
[RULE 6][MODEL 21]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bridgetower/modeling_bridgetower.py
@@ -387,11 +387,7 @@
 class BridgeTowerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 22]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/bros/modeling_bros.py
@@ -496,11 +496,7 @@
 class BrosIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 23]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/camembert/modeling_camembert.py
@@ -369,11 +369,7 @@
 class CamembertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 24]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/canine/modeling_canine.py
@@ -661,11 +661,7 @@
 class CanineIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 25]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/chinese_clip/modeling_chinese_clip.py
@@ -494,11 +494,7 @@
 class ChineseCLIPTextIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 26]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/clap/modeling_clap.py
@@ -1440,11 +1440,7 @@
 class ClapTextIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 27]
[RULE 6][MODEL 28]
[RULE 6][MODEL 29]
[RULE 6][MODEL 30]
[RULE 6][MODEL 31]
[RULE 6][MODEL 32]
[RULE 6][MODEL 33]
[RULE 6][MODEL 34]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/cpmant/modeling_cpmant.py
@@ -409,11 +409,7 @@
 class CpmAntIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 35]
[RULE 6][MODEL 36]
[RULE 6][MODEL 37]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta/modeling_deberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta/modeling_deberta.py
@@ -337,11 +337,7 @@
 class DebertaIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 38]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta_v2/modeling_deberta_v2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deberta_v2/modeling_deberta_v2.py
@@ -314,11 +314,7 @@
 class DebertaV2Intermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 39]
[RULE 6][MODEL 40]
[RULE 6][MODEL 41]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deit/modeling_deit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/deit/modeling_deit.py
@@ -259,11 +259,7 @@
 class DeiTIntermediate(nn.Module):
     def __init__(self, config: DeiTConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 42]
[RULE 6][MODEL 43]
[RULE 6][MODEL 44]
[RULE 6][MODEL 45]
[RULE 6][MODEL 46]
[RULE 6][MODEL 47]
[RULE 6][MODEL 48]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/dpt/modeling_dpt.py
@@ -430,11 +430,7 @@
 class DPTViTIntermediate(nn.Module):
     def __init__(self, config: DPTConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 49]
[RULE 6][MODEL 50]
[RULE 6][MODEL 51]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/electra/modeling_electra.py
@@ -416,11 +416,7 @@
 class ElectraIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 52]
[RULE 6][MODEL 53]
[RULE 6][MODEL 54]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/ernie/modeling_ernie.py
@@ -351,11 +351,7 @@
 class ErnieIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 55]
[RULE 6][MODEL 56]
[RULE 6][MODEL 57]
[RULE 6][MODEL 58]
[RULE 6][MODEL 59]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/flava/modeling_flava.py
@@ -561,11 +561,7 @@
 class FlavaIntermediate(nn.Module):
     def __init__(self, config: FlavaPossibleConfigs) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     # Copied from transformers.models.vit.modeling_vit.ViTIntermediate.forward
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
[RULE 6][MODEL 60]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/fnet/modeling_fnet.py
@@ -223,11 +223,7 @@
 class FNetIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 61]
[RULE 6][MODEL 62]
[RULE 6][MODEL 63]
[RULE 6][MODEL 64]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/git/modeling_git.py
@@ -336,11 +336,7 @@
 class GitIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 65]
[RULE 6][MODEL 66]
[RULE 6][MODEL 67]
[RULE 6][MODEL 68]
[RULE 6][MODEL 69]
[RULE 6][MODEL 70]
[RULE 6][MODEL 71]
[RULE 6][MODEL 72]
[RULE 6][MODEL 73]
[RULE 6][MODEL 74]
[RULE 6][MODEL 75]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/hubert/modeling_hubert.py
@@ -553,11 +553,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 76]
[RULE 6][MODEL 77]
[RULE 6][MODEL 78]
[RULE 6][MODEL 79]
[RULE 6][MODEL 80]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/instructblip/modeling_instructblip.py
@@ -765,11 +765,7 @@
 class InstructBlipQFormerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 81]
[RULE 6][MODEL 82]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlm/modeling_layoutlm.py
@@ -332,11 +332,7 @@
 class LayoutLMIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 83]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv2/modeling_layoutlmv2.py
@@ -246,11 +246,7 @@
 class LayoutLMv2Intermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 84]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/layoutlmv3/modeling_layoutlmv3.py
@@ -715,11 +715,7 @@
 class LayoutLMv3Intermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 85]
[RULE 6][MODEL 86]
[RULE 6][MODEL 87]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/lilt/modeling_lilt.py
@@ -399,11 +399,7 @@
 class LiltIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 88]
[RULE 6][MODEL 89]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/longformer/modeling_longformer.py
@@ -1197,11 +1197,7 @@
 class LongformerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 90]
[RULE 6][MODEL 91]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/luke/modeling_luke.py
@@ -681,11 +681,7 @@
 class LukeIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 92]
[RULE 6][MODEL 93]
[RULE 6][MODEL 94]
[RULE 6][MODEL 95]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/markuplm/modeling_markuplm.py
@@ -242,11 +242,7 @@
 class MarkupLMIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 96]
[RULE 6][MODEL 97]
[RULE 6][MODEL 98]
[RULE 6][MODEL 99]
[RULE 6][MODEL 100]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/megatron_bert/modeling_megatron_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/megatron_bert/modeling_megatron_bert.py
@@ -394,11 +394,7 @@
 class MegatronBertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 101]
[RULE 6][MODEL 102]
[RULE 6][MODEL 103]
[RULE 6][MODEL 104]
[RULE 6][MODEL 105]
[RULE 6][MODEL 106]
[RULE 6][MODEL 107]
[RULE 6][MODEL 108]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mpnet/modeling_mpnet.py
@@ -254,11 +254,7 @@
 class MPNetIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 109]
[RULE 6][MODEL 110]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mra/modeling_mra.py
@@ -688,11 +688,7 @@
 class MraIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 111]
[RULE 6][MODEL 112]
[RULE 6][MODEL 113]
[RULE 6][MODEL 114]
[RULE 6][MODEL 115]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nezha/modeling_nezha.py
@@ -423,11 +423,7 @@
 class NezhaIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 116]
[RULE 6][MODEL 117]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/nystromformer/modeling_nystromformer.py
@@ -290,11 +290,7 @@
 class NystromformerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 118]
[RULE 6][MODEL 119]
[RULE 6][MODEL 120]
[RULE 6][MODEL 121]
[RULE 6][MODEL 122]
[RULE 6][MODEL 123]
[RULE 6][MODEL 124]
[RULE 6][MODEL 125]
[RULE 6][MODEL 126]
[RULE 6][MODEL 127]
[RULE 6][MODEL 128]
[RULE 6][MODEL 129]
[RULE 6][MODEL 130]
[RULE 6][MODEL 131]
[RULE 6][MODEL 132]
[RULE 6][MODEL 133]
[RULE 6][MODEL 134]
[RULE 6][MODEL 135]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/realm/modeling_realm.py
@@ -431,11 +431,7 @@
 class RealmIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 136]
[RULE 6][MODEL 137]
[RULE 6][MODEL 138]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/rembert/modeling_rembert.py
@@ -386,11 +386,7 @@
 class RemBertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 139]
[RULE 6][MODEL 140]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta/modeling_roberta.py
@@ -355,11 +355,7 @@
 class RobertaIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 141]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py
@@ -358,11 +358,7 @@
     def __init__(self, config):
         super().__init__()
         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.LayerNorm(hidden_states)
[RULE 6][MODEL 142]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roc_bert/modeling_roc_bert.py
@@ -489,11 +489,7 @@
 class RoCBertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 143]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/roformer/modeling_roformer.py
@@ -415,11 +415,7 @@
 class RoFormerIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 144]
[RULE 6][MODEL 145]
[RULE 6][MODEL 146]
[RULE 6][MODEL 147]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew/modeling_sew.py
@@ -549,11 +549,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 148]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew_d/modeling_sew_d.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/sew_d/modeling_sew_d.py
@@ -945,11 +945,7 @@
 class SEWDIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 149]
[RULE 6][MODEL 150]
[RULE 6][MODEL 151]
[RULE 6][MODEL 152]
[RULE 6][MODEL 153]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/splinter/modeling_splinter.py
@@ -304,11 +304,7 @@
 class SplinterIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 154]
[RULE 6][MODEL 155]
[RULE 6][MODEL 156]
[RULE 6][MODEL 157]
[RULE 6][MODEL 158]
[RULE 6][MODEL 159]
[RULE 6][MODEL 160]
[RULE 6][MODEL 161]
[RULE 6][MODEL 162]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tapas/modeling_tapas.py
@@ -502,11 +502,7 @@
 class TapasIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 163]
[RULE 6][MODEL 164]
[RULE 6][MODEL 165]
[RULE 6][MODEL 166]
[RULE 6][MODEL 167]
[RULE 6][MODEL 168]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tvlt/modeling_tvlt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/tvlt/modeling_tvlt.py
@@ -465,11 +465,7 @@
 class TvltIntermediate(nn.Module):
     def __init__(self, config: TvltConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 169]
[RULE 6][MODEL 170]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech/modeling_unispeech.py
@@ -589,11 +589,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 171]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/unispeech_sat/modeling_unispeech_sat.py
@@ -603,11 +603,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 172]
[RULE 6][MODEL 173]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/videomae/modeling_videomae.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/videomae/modeling_videomae.py
@@ -336,11 +336,7 @@
 class VideoMAEIntermediate(nn.Module):
     def __init__(self, config: VideoMAEConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 174]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vilt/modeling_vilt.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vilt/modeling_vilt.py
@@ -438,11 +438,7 @@
 class ViltIntermediate(nn.Module):
     def __init__(self, config: ViltConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 175]
[RULE 6][MODEL 176]
[RULE 6][MODEL 177]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/visual_bert/modeling_visual_bert.py
@@ -326,11 +326,7 @@
 class VisualBertIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 178]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit/modeling_vit.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit/modeling_vit.py
@@ -302,11 +302,7 @@
 class ViTIntermediate(nn.Module):
     def __init__(self, config: ViTConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 179]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_hybrid/modeling_vit_hybrid.py
@@ -317,11 +317,7 @@
 class ViTHybridIntermediate(nn.Module):
     def __init__(self, config: ViTHybridConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 180]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_mae/modeling_vit_mae.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_mae/modeling_vit_mae.py
@@ -438,11 +438,7 @@
 class ViTMAEIntermediate(nn.Module):
     def __init__(self, config: ViTMAEConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 181]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_msn/modeling_vit_msn.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/vit_msn/modeling_vit_msn.py
@@ -289,11 +289,7 @@
 class ViTMSNIntermediate(nn.Module):
     def __init__(self, config: ViTMSNConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 182]
[RULE 6][MODEL 183]
[RULE 6][MODEL 184]
[RULE 6][MODEL 185]
[RULE 6][MODEL 186]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2/modeling_wav2vec2.py
@@ -654,11 +654,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 187]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py
@@ -557,11 +557,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 188]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/wavlm/modeling_wavlm.py
@@ -570,11 +570,7 @@
         super().__init__()
         self.intermediate_dropout = nn.Dropout(config.activation_dropout)
 
-        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.intermediate_dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.intermediate_dense.weight)
 
         self.output_dense = nn.Linear(config.intermediate_size, config.hidden_size)
         self.output_dropout = nn.Dropout(config.hidden_dropout)
[RULE 6][MODEL 189]
[RULE 6][MODEL 190]
[RULE 6][MODEL 191]
[RULE 6][MODEL 192]
[RULE 6][MODEL 193]
[RULE 6][MODEL 194]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta/modeling_xlm_roberta.py
@@ -356,11 +356,7 @@
 class XLMRobertaIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 195]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py
@@ -348,11 +348,7 @@
 class XLMRobertaXLIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 196]
[RULE 6][MODEL 197]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/xmod/modeling_xmod.py
@@ -355,11 +355,7 @@
 class XmodIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 198]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yolos/modeling_yolos.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yolos/modeling_yolos.py
@@ -372,11 +372,7 @@
 class YolosIntermediate(nn.Module):
     def __init__(self, config: YolosConfig) -> None:
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 6][MODEL 199]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/yoso/modeling_yoso.py
@@ -481,11 +481,7 @@
 class YosoIntermediate(nn.Module):
     def __init__(self, config):
         super().__init__()
-        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
-        if isinstance(config.hidden_act, str):
-            self.intermediate_act_fn = ACT2FN[config.hidden_act]
-        else:
-            self.intermediate_act_fn = config.hidden_act
+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size, bias=False); assert (config.hidden_act == gelu); self.intermediate_act_fn = torchscript_ops_FusedBiasGELU(config.intermediate_size, prev_weight=self.dense.weight)
 
     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
         hidden_states = self.dense(hidden_states)
[RULE 7]
[RULE 7][MODEL 1]
[RULE 7][MODEL 2]
[RULE 7][MODEL 3]
[RULE 7][MODEL 4]
[RULE 7][MODEL 5]
[RULE 7][MODEL 6]
[RULE 7][MODEL 7]
[RULE 7][MODEL 8]
[RULE 7][MODEL 9]
[RULE 7][MODEL 10]
[RULE 7][MODEL 11]
[RULE 7][MODEL 12]
[RULE 7][MODEL 13]
[RULE 7][MODEL 14]
[RULE 7][MODEL 15]
[RULE 7][MODEL 16]
[RULE 7][MODEL 17]
[RULE 7][MODEL 18]
[RULE 7][MODEL 19]
[RULE 7][MODEL 20]
[RULE 7][MODEL 21]
[RULE 7][MODEL 22]
[RULE 7][MODEL 23]
[RULE 7][MODEL 24]
[RULE 7][MODEL 25]
[RULE 7][MODEL 26]
[RULE 7][MODEL 27]
[RULE 7][MODEL 28]
[RULE 7][MODEL 29]
[RULE 7][MODEL 30]
[RULE 7][MODEL 31]
[RULE 7][MODEL 32]
[RULE 7][MODEL 33]
[RULE 7][MODEL 34]
[RULE 7][MODEL 35]
[RULE 7][MODEL 36]
[RULE 7][MODEL 37]
[RULE 7][MODEL 38]
[RULE 7][MODEL 39]
[RULE 7][MODEL 40]
[RULE 7][MODEL 41]
[RULE 7][MODEL 42]
[RULE 7][MODEL 43]
[RULE 7][MODEL 44]
[RULE 7][MODEL 45]
[RULE 7][MODEL 46]
[RULE 7][MODEL 47]
[RULE 7][MODEL 48]
[RULE 7][MODEL 49]
[RULE 7][MODEL 50]
[RULE 7][MODEL 51]
[RULE 7][MODEL 52]
[RULE 7][MODEL 53]
[RULE 7][MODEL 54]
[RULE 7][MODEL 55]
[RULE 7][MODEL 56]
[RULE 7][MODEL 57]
[RULE 7][MODEL 58]
[RULE 7][MODEL 59]
[RULE 7][MODEL 60]
[RULE 7][MODEL 61]
[RULE 7][MODEL 62]
[RULE 7][MODEL 63]
[RULE 7][MODEL 64]
[RULE 7][MODEL 65]
[RULE 7][MODEL 66]
[RULE 7][MODEL 67]
[RULE 7][MODEL 68]
[RULE 7][MODEL 69]
[RULE 7][MODEL 70]
[RULE 7][MODEL 71]
[RULE 7][MODEL 72]
[RULE 7][MODEL 73]
[RULE 7][MODEL 74]
[RULE 7][MODEL 75]
[RULE 7][MODEL 76]
[RULE 7][MODEL 77]
[RULE 7][MODEL 78]
[RULE 7][MODEL 79]
[RULE 7][MODEL 80]
[RULE 7][MODEL 81]
[RULE 7][MODEL 82]
[RULE 7][MODEL 83]
[RULE 7][MODEL 84]
[RULE 7][MODEL 85]
[RULE 7][MODEL 86]
[RULE 7][MODEL 87]
[RULE 7][MODEL 88]
[RULE 7][MODEL 89]
[RULE 7][MODEL 90]
[RULE 7][MODEL 91]
[RULE 7][MODEL 92]
[RULE 7][MODEL 93]
[RULE 7][MODEL 94]
[RULE 7][MODEL 95]
[RULE 7][MODEL 96]
[RULE 7][MODEL 97]
[RULE 7][MODEL 98]
[RULE 7][MODEL 99]
[RULE 7][MODEL 100]
[RULE 7][MODEL 101]
[RULE 7][MODEL 102]
[RULE 7][MODEL 103]
[RULE 7][MODEL 104]
[RULE 7][MODEL 105]
[RULE 7][MODEL 106]
[RULE 7][MODEL 107]
[RULE 7][MODEL 108]
[RULE 7][MODEL 109]
[RULE 7][MODEL 110]
[RULE 7][MODEL 111]
[RULE 7][MODEL 112]
[RULE 7][MODEL 113]
[RULE 7][MODEL 114]
[RULE 7][MODEL 115]
[RULE 7][MODEL 116]
[RULE 7][MODEL 117]
[RULE 7][MODEL 118]
[RULE 7][MODEL 119]
[RULE 7][MODEL 120]
[RULE 7][MODEL 121]
[RULE 7][MODEL 122]
[RULE 7][MODEL 123]
[RULE 7][MODEL 124]
[RULE 7][MODEL 125]
[RULE 7][MODEL 126]
[RULE 7][MODEL 127]
[RULE 7][MODEL 128]
[RULE 7][MODEL 129]
[RULE 7][MODEL 130]
[RULE 7][MODEL 131]
[RULE 7][MODEL 132]
[RULE 7][MODEL 133]
[RULE 7][MODEL 134]
[RULE 7][MODEL 135]
[RULE 7][MODEL 136]
[RULE 7][MODEL 137]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/regnet/modeling_regnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/regnet/modeling_regnet.py
@@ -113,8 +113,7 @@
 
     def __init__(self, in_channels: int, out_channels: int, stride: int = 2):
         super().__init__()
-        self.convolution = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)
-        self.normalization = nn.BatchNorm2d(out_channels)
+        self.conv_batchnorm = torchscript_ops_FusedConv2dBatchNorm2d(in_channels, out_channels, 1, stride=stride, bias=False)
 
     def forward(self, input: Tensor) -> Tensor:
         hidden_state = self.convolution(input)
[RULE 7][MODEL 138]
[RULE 7][MODEL 139]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/resnet/modeling_resnet.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/resnet/modeling_resnet.py
@@ -109,8 +109,7 @@
 
     def __init__(self, in_channels: int, out_channels: int, stride: int = 2):
         super().__init__()
-        self.convolution = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)
-        self.normalization = nn.BatchNorm2d(out_channels)
+        self.conv_batchnorm = torchscript_ops_FusedConv2dBatchNorm2d(in_channels, out_channels, 1, stride=stride, bias=False)
 
     def forward(self, input: Tensor) -> Tensor:
         hidden_state = self.convolution(input)
[RULE 7][MODEL 140]
[RULE 7][MODEL 141]
[RULE 7][MODEL 142]
[RULE 7][MODEL 143]
[RULE 7][MODEL 144]
[RULE 7][MODEL 145]
[RULE 7][MODEL 146]
[RULE 7][MODEL 147]
[RULE 7][MODEL 148]
[RULE 7][MODEL 149]
[RULE 7][MODEL 150]
[RULE 7][MODEL 151]
[RULE 7][MODEL 152]
[RULE 7][MODEL 153]
[RULE 7][MODEL 154]
[RULE 7][MODEL 155]
[RULE 7][MODEL 156]
[RULE 7][MODEL 157]
[RULE 7][MODEL 158]
[RULE 7][MODEL 159]
[RULE 7][MODEL 160]
[RULE 7][MODEL 161]
[RULE 7][MODEL 162]
[RULE 7][MODEL 163]
[RULE 7][MODEL 164]
[RULE 7][MODEL 165]
[RULE 7][MODEL 166]
[RULE 7][MODEL 167]
[RULE 7][MODEL 168]
[RULE 7][MODEL 169]
[RULE 7][MODEL 170]
[RULE 7][MODEL 171]
[RULE 7][MODEL 172]
[RULE 7][MODEL 173]
[RULE 7][MODEL 174]
[RULE 7][MODEL 175]
[RULE 7][MODEL 176]
[RULE 7][MODEL 177]
[RULE 7][MODEL 178]
[RULE 7][MODEL 179]
[RULE 7][MODEL 180]
[RULE 7][MODEL 181]
[RULE 7][MODEL 182]
[RULE 7][MODEL 183]
[RULE 7][MODEL 184]
[RULE 7][MODEL 185]
[RULE 7][MODEL 186]
[RULE 7][MODEL 187]
[RULE 7][MODEL 188]
[RULE 7][MODEL 189]
[RULE 7][MODEL 190]
[RULE 7][MODEL 191]
[RULE 7][MODEL 192]
[RULE 7][MODEL 193]
[RULE 7][MODEL 194]
[RULE 7][MODEL 195]
[RULE 7][MODEL 196]
[RULE 7][MODEL 197]
[RULE 7][MODEL 198]
[RULE 7][MODEL 199]
[RULE 8]
[RULE 8][MODEL 1]
[RULE 8][MODEL 2]
[RULE 8][MODEL 3]
[RULE 8][MODEL 4]
[RULE 8][MODEL 5]
[RULE 8][MODEL 6]
[RULE 8][MODEL 7]
[RULE 8][MODEL 8]
[RULE 8][MODEL 9]
[RULE 8][MODEL 10]
[RULE 8][MODEL 11]
[RULE 8][MODEL 12]
[RULE 8][MODEL 13]
[RULE 8][MODEL 14]
[RULE 8][MODEL 15]
[RULE 8][MODEL 16]
[RULE 8][MODEL 17]
[RULE 8][MODEL 18]
[RULE 8][MODEL 19]
[RULE 8][MODEL 20]
[RULE 8][MODEL 21]
[RULE 8][MODEL 22]
[RULE 8][MODEL 23]
[RULE 8][MODEL 24]
[RULE 8][MODEL 25]
[RULE 8][MODEL 26]
[RULE 8][MODEL 27]
[RULE 8][MODEL 28]
[RULE 8][MODEL 29]
[RULE 8][MODEL 30]
[RULE 8][MODEL 31]
[RULE 8][MODEL 32]
[RULE 8][MODEL 33]
[RULE 8][MODEL 34]
[RULE 8][MODEL 35]
[RULE 8][MODEL 36]
[RULE 8][MODEL 37]
[RULE 8][MODEL 38]
[RULE 8][MODEL 39]
[RULE 8][MODEL 40]
[RULE 8][MODEL 41]
[RULE 8][MODEL 42]
[RULE 8][MODEL 43]
[RULE 8][MODEL 44]
[RULE 8][MODEL 45]
[RULE 8][MODEL 46]
[RULE 8][MODEL 47]
[RULE 8][MODEL 48]
[RULE 8][MODEL 49]
[RULE 8][MODEL 50]
[RULE 8][MODEL 51]
[RULE 8][MODEL 52]
[RULE 8][MODEL 53]
[RULE 8][MODEL 54]
[RULE 8][MODEL 55]
[RULE 8][MODEL 56]
[RULE 8][MODEL 57]
[RULE 8][MODEL 58]
[RULE 8][MODEL 59]
[RULE 8][MODEL 60]
[RULE 8][MODEL 61]
[RULE 8][MODEL 62]
[RULE 8][MODEL 63]
[RULE 8][MODEL 64]
[RULE 8][MODEL 65]
[RULE 8][MODEL 66]
[RULE 8][MODEL 67]
[RULE 8][MODEL 68]
[RULE 8][MODEL 69]
[RULE 8][MODEL 70]
[RULE 8][MODEL 71]
[RULE 8][MODEL 72]
[RULE 8][MODEL 73]
[RULE 8][MODEL 74]
[RULE 8][MODEL 75]
[RULE 8][MODEL 76]
[RULE 8][MODEL 77]
[RULE 8][MODEL 78]
[RULE 8][MODEL 79]
[RULE 8][MODEL 80]
[RULE 8][MODEL 81]
[RULE 8][MODEL 82]
[RULE 8][MODEL 83]
[RULE 8][MODEL 84]
[RULE 8][MODEL 85]
[RULE 8][MODEL 86]
[RULE 8][MODEL 87]
[RULE 8][MODEL 88]
[RULE 8][MODEL 89]
[RULE 8][MODEL 90]
[RULE 8][MODEL 91]
[RULE 8][MODEL 92]
[RULE 8][MODEL 93]
[RULE 8][MODEL 94]
[RULE 8][MODEL 95]
[RULE 8][MODEL 96]
[RULE 8][MODEL 97]
[RULE 8][MODEL 98]
[RULE 8][MODEL 99]
[RULE 8][MODEL 100]
[RULE 8][MODEL 101]
[RULE 8][MODEL 102]
[RULE 8][MODEL 103]
[RULE 8][MODEL 104]
[RULE 8][MODEL 105]
[RULE 8][MODEL 106]
[RULE 8][MODEL 107]
[RULE 8][MODEL 108]
[RULE 8][MODEL 109]
[RULE 8][MODEL 110]
[RULE 8][MODEL 111]
[RULE 8][MODEL 112]
[RULE 8][MODEL 113]
[RULE 8][MODEL 114]
[RULE 8][MODEL 115]
[RULE 8][MODEL 116]
[RULE 8][MODEL 117]
[RULE 8][MODEL 118]
[RULE 8][MODEL 119]
[RULE 8][MODEL 120]
[RULE 8][MODEL 121]
[RULE 8][MODEL 122]
[RULE 8][MODEL 123]
[RULE 8][MODEL 124]
[RULE 8][MODEL 125]
[RULE 8][MODEL 126]
[RULE 8][MODEL 127]
[RULE 8][MODEL 128]
[RULE 8][MODEL 129]
[RULE 8][MODEL 130]
[RULE 8][MODEL 131]
[RULE 8][MODEL 132]
[RULE 8][MODEL 133]
[RULE 8][MODEL 134]
[RULE 8][MODEL 135]
[RULE 8][MODEL 136]
[RULE 8][MODEL 137]
[RULE 8][MODEL 138]
[RULE 8][MODEL 139]
[RULE 8][MODEL 140]
[RULE 8][MODEL 141]
[RULE 8][MODEL 142]
[RULE 8][MODEL 143]
[RULE 8][MODEL 144]
[RULE 8][MODEL 145]
[RULE 8][MODEL 146]
[RULE 8][MODEL 147]
[RULE 8][MODEL 148]
[RULE 8][MODEL 149]
[RULE 8][MODEL 150]
[RULE 8][MODEL 151]
[RULE 8][MODEL 152]
[RULE 8][MODEL 153]
[RULE 8][MODEL 154]
[RULE 8][MODEL 155]
[RULE 8][MODEL 156]
[RULE 8][MODEL 157]
[RULE 8][MODEL 158]
[RULE 8][MODEL 159]
[RULE 8][MODEL 160]
[RULE 8][MODEL 161]
[RULE 8][MODEL 162]
[RULE 8][MODEL 163]
[RULE 8][MODEL 164]
[RULE 8][MODEL 165]
[RULE 8][MODEL 166]
[RULE 8][MODEL 167]
[RULE 8][MODEL 168]
[RULE 8][MODEL 169]
[RULE 8][MODEL 170]
[RULE 8][MODEL 171]
[RULE 8][MODEL 172]
[RULE 8][MODEL 173]
[RULE 8][MODEL 174]
[RULE 8][MODEL 175]
[RULE 8][MODEL 176]
[RULE 8][MODEL 177]
[RULE 8][MODEL 178]
[RULE 8][MODEL 179]
[RULE 8][MODEL 180]
[RULE 8][MODEL 181]
[RULE 8][MODEL 182]
[RULE 8][MODEL 183]
[RULE 8][MODEL 184]
[RULE 8][MODEL 185]
[RULE 8][MODEL 186]
[RULE 8][MODEL 187]
[RULE 8][MODEL 188]
[RULE 8][MODEL 189]
[RULE 8][MODEL 190]
[RULE 8][MODEL 191]
[RULE 8][MODEL 192]
[RULE 8][MODEL 193]
[RULE 8][MODEL 194]
[RULE 8][MODEL 195]
[RULE 8][MODEL 196]
[RULE 8][MODEL 197]
[RULE 8][MODEL 198]
[RULE 8][MODEL 199]
[RULE 9_1]
[RULE 9_1][MODEL 1]
[RULE 9_1][MODEL 2]
[RULE 9_1][MODEL 3]
[RULE 9_1][MODEL 4]
[RULE 9_1][MODEL 5]
[RULE 9_1][MODEL 6]
[RULE 9_1][MODEL 7]
[RULE 9_1][MODEL 8]
[RULE 9_1][MODEL 9]
[RULE 9_1][MODEL 10]
[RULE 9_1][MODEL 11]
[RULE 9_1][MODEL 12]
[RULE 9_1][MODEL 13]
[RULE 9_1][MODEL 14]
[RULE 9_1][MODEL 15]
[RULE 9_1][MODEL 16]
[RULE 9_1][MODEL 17]
[RULE 9_1][MODEL 18]
[RULE 9_1][MODEL 19]
[RULE 9_1][MODEL 20]
[RULE 9_1][MODEL 21]
[RULE 9_1][MODEL 22]
[RULE 9_1][MODEL 23]
[RULE 9_1][MODEL 24]
[RULE 9_1][MODEL 25]
[RULE 9_1][MODEL 26]
[RULE 9_1][MODEL 27]
[RULE 9_1][MODEL 28]
[RULE 9_1][MODEL 29]
[RULE 9_1][MODEL 30]
[RULE 9_1][MODEL 31]
[RULE 9_1][MODEL 32]
[RULE 9_1][MODEL 33]
[RULE 9_1][MODEL 34]
[RULE 9_1][MODEL 35]
[RULE 9_1][MODEL 36]
[RULE 9_1][MODEL 37]
[RULE 9_1][MODEL 38]
[RULE 9_1][MODEL 39]
[RULE 9_1][MODEL 40]
[RULE 9_1][MODEL 41]
[RULE 9_1][MODEL 42]
[RULE 9_1][MODEL 43]
[RULE 9_1][MODEL 44]
[RULE 9_1][MODEL 45]
[RULE 9_1][MODEL 46]
[RULE 9_1][MODEL 47]
[RULE 9_1][MODEL 48]
[RULE 9_1][MODEL 49]
[RULE 9_1][MODEL 50]
[RULE 9_1][MODEL 51]
[RULE 9_1][MODEL 52]
[RULE 9_1][MODEL 53]
[RULE 9_1][MODEL 54]
[RULE 9_1][MODEL 55]
[RULE 9_1][MODEL 56]
[RULE 9_1][MODEL 57]
[RULE 9_1][MODEL 58]
[RULE 9_1][MODEL 59]
[RULE 9_1][MODEL 60]
[RULE 9_1][MODEL 61]
[RULE 9_1][MODEL 62]
[RULE 9_1][MODEL 63]
[RULE 9_1][MODEL 64]
[RULE 9_1][MODEL 65]
[RULE 9_1][MODEL 66]
[RULE 9_1][MODEL 67]
[RULE 9_1][MODEL 68]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
@@ -221,9 +221,7 @@
         use_cache=False,
         output_attentions=False,
     ):
-        query = self.q_proj(hidden_states)
-        key = self.k_proj(hidden_states)
-        value = self.v_proj(hidden_states)
+        (query, key, value) = self.qkv(hidden_states)
 
         query = self._split_heads(query, self.num_heads, self.head_dim)
         key = self._split_heads(key, self.num_heads, self.head_dim)
[RULE 9_1][MODEL 69]
[RULE 9_1][MODEL 70]
[RULE 9_1][MODEL 71]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
@@ -204,9 +204,7 @@
         Tuple[torch.Tensor, Tuple[torch.Tensor]],
         Optional[Tuple[torch.Tensor, Tuple[torch.Tensor], Tuple[torch.Tensor, ...]]],
     ]:
-        query = self.q_proj(hidden_states)
-        key = self.k_proj(hidden_states)
-        value = self.v_proj(hidden_states)
+        (query, key, value) = self.qkv(hidden_states)
 
         query = self._split_heads(query, self.num_attention_heads, self.head_dim, True)
         key = self._split_heads(key, self.num_attention_heads, self.head_dim, True)
[RULE 9_1][MODEL 72]
[RULE 9_1][MODEL 73]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/graphormer/modeling_graphormer.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/graphormer/modeling_graphormer.py
@@ -412,9 +412,7 @@
                         "The batch shape does not match the key or value shapes provided to the attention."
                     )
 
-        q = self.q_proj(query)
-        k = self.k_proj(query)
-        v = self.v_proj(query)
+        (q, k, v) = self.qkv(query)
 
         q *= self.scaling
 
[RULE 9_1][MODEL 74]
[RULE 9_1][MODEL 75]
[RULE 9_1][MODEL 76]
[RULE 9_1][MODEL 77]
[RULE 9_1][MODEL 78]
[RULE 9_1][MODEL 79]
[RULE 9_1][MODEL 80]
[RULE 9_1][MODEL 81]
[RULE 9_1][MODEL 82]
[RULE 9_1][MODEL 83]
[RULE 9_1][MODEL 84]
[RULE 9_1][MODEL 85]
[RULE 9_1][MODEL 86]
[RULE 9_1][MODEL 87]
[RULE 9_1][MODEL 88]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/llama/modeling_llama.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/llama/modeling_llama.py
@@ -344,9 +344,7 @@
             value_states = torch.cat(value_states, dim=-1)
 
         else:
-            query_states = self.q_proj(hidden_states)
-            key_states = self.k_proj(hidden_states)
-            value_states = self.v_proj(hidden_states)
+            (query_states, key_states, value_states) = self.qkv(hidden_states)
 
         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
@@ -432,9 +430,7 @@
 
         bsz, q_len, _ = hidden_states.size()
 
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
+        (query_states, key_states, value_states) = self.qkv(hidden_states)
 
         # Flash attention requires the input to have the shape
         # batch_size x seq_length x head_dime x hidden_dim
[RULE 9_1][MODEL 89]
[RULE 9_1][MODEL 90]
[RULE 9_1][MODEL 91]
[RULE 9_1][MODEL 92]
[RULE 9_1][MODEL 93]
[RULE 9_1][MODEL 94]
[RULE 9_1][MODEL 95]
[RULE 9_1][MODEL 96]
[RULE 9_1][MODEL 97]
[RULE 9_1][MODEL 98]
[RULE 9_1][MODEL 99]
[RULE 9_1][MODEL 100]
[RULE 9_1][MODEL 101]
[RULE 9_1][MODEL 102]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mistral/modeling_mistral.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/mistral/modeling_mistral.py
@@ -255,9 +255,7 @@
     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
         bsz, q_len, _ = hidden_states.size()
 
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
+        (query_states, key_states, value_states) = self.qkv(hidden_states)
 
         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
@@ -336,9 +334,7 @@
     ):
         bsz, q_len, _ = hidden_states.size()
 
-        query_states = self.q_proj(hidden_states)
-        key_states = self.k_proj(hidden_states)
-        value_states = self.v_proj(hidden_states)
+        (query_states, key_states, value_states) = self.qkv(hidden_states)
 
         query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
         key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
[RULE 9_1][MODEL 103]
[RULE 9_1][MODEL 104]
[RULE 9_1][MODEL 105]
[RULE 9_1][MODEL 106]
[RULE 9_1][MODEL 107]
[RULE 9_1][MODEL 108]
[RULE 9_1][MODEL 109]
[RULE 9_1][MODEL 110]
[RULE 9_1][MODEL 111]
[RULE 9_1][MODEL 112]
[RULE 9_1][MODEL 113]
[RULE 9_1][MODEL 114]
[RULE 9_1][MODEL 115]
[RULE 9_1][MODEL 116]
[RULE 9_1][MODEL 117]
[RULE 9_1][MODEL 118]
[RULE 9_1][MODEL 119]
[RULE 9_1][MODEL 120]
[RULE 9_1][MODEL 121]
[RULE 9_1][MODEL 122]
[RULE 9_1][MODEL 123]
[RULE 9_1][MODEL 124]
[RULE 9_1][MODEL 125]
[RULE 9_1][MODEL 126]
[RULE 9_1][MODEL 127]
[RULE 9_1][MODEL 128]
[RULE 9_1][MODEL 129]
[RULE 9_1][MODEL 130]
[RULE 9_1][MODEL 131]
[RULE 9_1][MODEL 132]
[RULE 9_1][MODEL 133]
[RULE 9_1][MODEL 134]
[RULE 9_1][MODEL 135]
[RULE 9_1][MODEL 136]
[RULE 9_1][MODEL 137]
[RULE 9_1][MODEL 138]
[RULE 9_1][MODEL 139]
[RULE 9_1][MODEL 140]
[RULE 9_1][MODEL 141]
[RULE 9_1][MODEL 142]
[RULE 9_1][MODEL 143]
[RULE 9_1][MODEL 144]
[RULE 9_1][MODEL 145]
[RULE 9_1][MODEL 146]
[RULE 9_1][MODEL 147]
[RULE 9_1][MODEL 148]
[RULE 9_1][MODEL 149]
[RULE 9_1][MODEL 150]
[RULE 9_1][MODEL 151]
[RULE 9_1][MODEL 152]
[RULE 9_1][MODEL 153]
[RULE 9_1][MODEL 154]
[RULE 9_1][MODEL 155]
[RULE 9_1][MODEL 156]
[RULE 9_1][MODEL 157]
[RULE 9_1][MODEL 158]
[RULE 9_1][MODEL 159]
[RULE 9_1][MODEL 160]
[RULE 9_1][MODEL 161]
[RULE 9_1][MODEL 162]
[RULE 9_1][MODEL 163]
[RULE 9_1][MODEL 164]
[RULE 9_1][MODEL 165]
[RULE 9_1][MODEL 166]
[RULE 9_1][MODEL 167]
[RULE 9_1][MODEL 168]
[RULE 9_1][MODEL 169]
[RULE 9_1][MODEL 170]
[RULE 9_1][MODEL 171]
[RULE 9_1][MODEL 172]
[RULE 9_1][MODEL 173]
[RULE 9_1][MODEL 174]
[RULE 9_1][MODEL 175]
[RULE 9_1][MODEL 176]
[RULE 9_1][MODEL 177]
[RULE 9_1][MODEL 178]
[RULE 9_1][MODEL 179]
[RULE 9_1][MODEL 180]
[RULE 9_1][MODEL 181]
[RULE 9_1][MODEL 182]
[RULE 9_1][MODEL 183]
[RULE 9_1][MODEL 184]
[RULE 9_1][MODEL 185]
[RULE 9_1][MODEL 186]
[RULE 9_1][MODEL 187]
[RULE 9_1][MODEL 188]
[RULE 9_1][MODEL 189]
[RULE 9_1][MODEL 190]
[RULE 9_1][MODEL 191]
[RULE 9_1][MODEL 192]
[RULE 9_1][MODEL 193]
[RULE 9_1][MODEL 194]
[RULE 9_1][MODEL 195]
[RULE 9_1][MODEL 196]
[RULE 9_1][MODEL 197]
[RULE 9_1][MODEL 198]
[RULE 9_1][MODEL 199]
[RULE 9_2]
[RULE 9_2][MODEL 1]
[RULE 9_2][MODEL 2]
[RULE 9_2][MODEL 3]
[RULE 9_2][MODEL 4]
[RULE 9_2][MODEL 5]
[RULE 9_2][MODEL 6]
[RULE 9_2][MODEL 7]
[RULE 9_2][MODEL 8]
[RULE 9_2][MODEL 9]
[RULE 9_2][MODEL 10]
[RULE 9_2][MODEL 11]
[RULE 9_2][MODEL 12]
[RULE 9_2][MODEL 13]
[RULE 9_2][MODEL 14]
[RULE 9_2][MODEL 15]
[RULE 9_2][MODEL 16]
[RULE 9_2][MODEL 17]
[RULE 9_2][MODEL 18]
[RULE 9_2][MODEL 19]
[RULE 9_2][MODEL 20]
[RULE 9_2][MODEL 21]
[RULE 9_2][MODEL 22]
[RULE 9_2][MODEL 23]
[RULE 9_2][MODEL 24]
[RULE 9_2][MODEL 25]
[RULE 9_2][MODEL 26]
[RULE 9_2][MODEL 27]
[RULE 9_2][MODEL 28]
[RULE 9_2][MODEL 29]
[RULE 9_2][MODEL 30]
[RULE 9_2][MODEL 31]
[RULE 9_2][MODEL 32]
[RULE 9_2][MODEL 33]
[RULE 9_2][MODEL 34]
[RULE 9_2][MODEL 35]
[RULE 9_2][MODEL 36]
[RULE 9_2][MODEL 37]
[RULE 9_2][MODEL 38]
[RULE 9_2][MODEL 39]
[RULE 9_2][MODEL 40]
[RULE 9_2][MODEL 41]
[RULE 9_2][MODEL 42]
[RULE 9_2][MODEL 43]
[RULE 9_2][MODEL 44]
[RULE 9_2][MODEL 45]
[RULE 9_2][MODEL 46]
[RULE 9_2][MODEL 47]
[RULE 9_2][MODEL 48]
[RULE 9_2][MODEL 49]
[RULE 9_2][MODEL 50]
[RULE 9_2][MODEL 51]
[RULE 9_2][MODEL 52]
[RULE 9_2][MODEL 53]
[RULE 9_2][MODEL 54]
[RULE 9_2][MODEL 55]
[RULE 9_2][MODEL 56]
[RULE 9_2][MODEL 57]
[RULE 9_2][MODEL 58]
[RULE 9_2][MODEL 59]
[RULE 9_2][MODEL 60]
[RULE 9_2][MODEL 61]
[RULE 9_2][MODEL 62]
[RULE 9_2][MODEL 63]
[RULE 9_2][MODEL 64]
[RULE 9_2][MODEL 65]
[RULE 9_2][MODEL 66]
[RULE 9_2][MODEL 67]
[RULE 9_2][MODEL 68]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gpt_neo/modeling_gpt_neo.py
@@ -160,9 +160,7 @@
                 f" {self.num_heads})."
             )
 
-        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
+        from slapo.op.linear import FusedQKV as linear_FusedQKV; self.qkv = linear_FusedQKV(self.embed_dim, self.num_heads, 1)
         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)
 
     def _split_heads(self, tensor, num_heads, attn_head_size):
[RULE 9_2][MODEL 69]
[RULE 9_2][MODEL 70]
[RULE 9_2][MODEL 71]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/gptj/modeling_gptj.py
@@ -106,9 +106,7 @@
             )
         self.scale_attn = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(torch.get_default_dtype())
 
-        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
-        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
+        from slapo.op.linear import FusedQKV as linear_FusedQKV; self.qkv = linear_FusedQKV(self.embed_dim, self.num_heads, 1)
         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)
         self.rotary_dim = config.rotary_dim
         pos_embd_dim = self.rotary_dim or self.embed_dim
[RULE 9_2][MODEL 72]
[RULE 9_2][MODEL 73]
[RULE 9_2][MODEL 74]
[RULE 9_2][MODEL 75]
[RULE 9_2][MODEL 76]
[RULE 9_2][MODEL 77]
[RULE 9_2][MODEL 78]
[RULE 9_2][MODEL 79]
[RULE 9_2][MODEL 80]
[RULE 9_2][MODEL 81]
[RULE 9_2][MODEL 82]
[RULE 9_2][MODEL 83]
[RULE 9_2][MODEL 84]
[RULE 9_2][MODEL 85]
[RULE 9_2][MODEL 86]
[RULE 9_2][MODEL 87]
[RULE 9_2][MODEL 88]
[RULE 9_2][MODEL 89]
[RULE 9_2][MODEL 90]
[RULE 9_2][MODEL 91]
[RULE 9_2][MODEL 92]
[RULE 9_2][MODEL 93]
[RULE 9_2][MODEL 94]
[RULE 9_2][MODEL 95]
[RULE 9_2][MODEL 96]
[RULE 9_2][MODEL 97]
[RULE 9_2][MODEL 98]
[RULE 9_2][MODEL 99]
[RULE 9_2][MODEL 100]
[RULE 9_2][MODEL 101]
[RULE 9_2][MODEL 102]
[RULE 9_2][MODEL 103]
[RULE 9_2][MODEL 104]
[RULE 9_2][MODEL 105]
[RULE 9_2][MODEL 106]
[RULE 9_2][MODEL 107]
[RULE 9_2][MODEL 108]
[RULE 9_2][MODEL 109]
[RULE 9_2][MODEL 110]
[RULE 9_2][MODEL 111]
[RULE 9_2][MODEL 112]
[RULE 9_2][MODEL 113]
[RULE 9_2][MODEL 114]
[RULE 9_2][MODEL 115]
[RULE 9_2][MODEL 116]
[RULE 9_2][MODEL 117]
[RULE 9_2][MODEL 118]
[RULE 9_2][MODEL 119]
[RULE 9_2][MODEL 120]
[RULE 9_2][MODEL 121]
[RULE 9_2][MODEL 122]
[RULE 9_2][MODEL 123]
[RULE 9_2][MODEL 124]
[RULE 9_2][MODEL 125]
[RULE 9_2][MODEL 126]
[RULE 9_2][MODEL 127]
--- /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pix2struct/modeling_pix2struct.py
+++ /home/ubuntu/pytorch-opt/adopter/models/huggingface_transformers/src/transformers/models/pix2struct/modeling_pix2struct.py
@@ -717,9 +717,7 @@
         self.inner_dim = self.n_heads * self.key_value_proj_dim
 
         # Mesh TensorFlow initialization to avoid scaling before softmax
-        self.query = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
-        self.key = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
-        self.value = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
+        from slapo.op.linear import FusedQKV as linear_FusedQKV; self.qkv = linear_FusedQKV(self.hidden_size, self.num_heads, 1)
         self.output = nn.Linear(self.hidden_size, self.hidden_size, bias=False)
 
         if self.has_relative_attention_bias:
[RULE 9_2][MODEL 128]
[RULE 9_2][MODEL 129]
[RULE 9_2][MODEL 130]
[RULE 9_2][MODEL 131]
[RULE 9_2][MODEL 132]
[RULE 9_2][MODEL 133]
[RULE 9_2][MODEL 134]
[RULE 9_2][MODEL 135]
[RULE 9_2][MODEL 136]
[RULE 9_2][MODEL 137]
[RULE 9_2][MODEL 138]
[RULE 9_2][MODEL 139]
[RULE 9_2][MODEL 140]
[RULE 9_2][MODEL 141]
[RULE 9_2][MODEL 142]
[RULE 9_2][MODEL 143]
[RULE 9_2][MODEL 144]
[RULE 9_2][MODEL 145]
[RULE 9_2][MODEL 146]
[RULE 9_2][MODEL 147]
[RULE 9_2][MODEL 148]
[RULE 9_2][MODEL 149]
[RULE 9_2][MODEL 150]
[RULE 9_2][MODEL 151]
[RULE 9_2][MODEL 152]
[RULE 9_2][MODEL 153]
[RULE 9_2][MODEL 154]
[RULE 9_2][MODEL 155]
[RULE 9_2][MODEL 156]
[RULE 9_2][MODEL 157]
[RULE 9_2][MODEL 158]
[RULE 9_2][MODEL 159]
[RULE 9_2][MODEL 160]
[RULE 9_2][MODEL 161]
[RULE 9_2][MODEL 162]
[RULE 9_2][MODEL 163]
[RULE 9_2][MODEL 164]
[RULE 9_2][MODEL 165]
[RULE 9_2][MODEL 166]
[RULE 9_2][MODEL 167]
[RULE 9_2][MODEL 168]
[RULE 9_2][MODEL 169]
[RULE 9_2][MODEL 170]
[RULE 9_2][MODEL 171]
[RULE 9_2][MODEL 172]
[RULE 9_2][MODEL 173]
[RULE 9_2][MODEL 174]
[RULE 9_2][MODEL 175]
[RULE 9_2][MODEL 176]
[RULE 9_2][MODEL 177]
[RULE 9_2][MODEL 178]
[RULE 9_2][MODEL 179]
[RULE 9_2][MODEL 180]
[RULE 9_2][MODEL 181]
[RULE 9_2][MODEL 182]
[RULE 9_2][MODEL 183]
[RULE 9_2][MODEL 184]
[RULE 9_2][MODEL 185]
[RULE 9_2][MODEL 186]
[RULE 9_2][MODEL 187]
[RULE 9_2][MODEL 188]
[RULE 9_2][MODEL 189]
[RULE 9_2][MODEL 190]
[RULE 9_2][MODEL 191]
[RULE 9_2][MODEL 192]
[RULE 9_2][MODEL 193]
[RULE 9_2][MODEL 194]
[RULE 9_2][MODEL 195]
[RULE 9_2][MODEL 196]
[RULE 9_2][MODEL 197]
[RULE 9_2][MODEL 198]
[RULE 9_2][MODEL 199]

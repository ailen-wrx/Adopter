{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a120bbad",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "This notebook shows the benchmark results of all covered ops.\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de4e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def build_package(name, url, commit=None, deps=True):\n",
    "    import importlib\n",
    "    import os, sys\n",
    "    if importlib.util.find_spec(name) is None:\n",
    "        os.system(f\"git clone {url} {name} || true\")\n",
    "        if commit is not None:\n",
    "            os.system(f\"cd {name}; git checkout {commit}\")\n",
    "        os.system(f\"cd {name}; git submodule update --init --recursive\")\n",
    "        no_deps = \"\"\n",
    "        if deps:\n",
    "            os.system(f\"cd {name}; pip3 install -r requirements.txt || true\")\n",
    "        else:\n",
    "            no_deps = \"--no-deps\"\n",
    "        clear_output()\n",
    "        os.system(f'cd {name}; pip3 install -e \".[dev]\" {no_deps}')\n",
    "\n",
    "build_package(\"transformers\", \"https://github.com/huggingface/transformers.git\", deps=False)\n",
    "build_package(\"xformers\", \"https://github.com/facebookresearch/xformers.git\")\n",
    "build_package(\"epoi\", \"https://github.com/comaniac/epoi.git\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606041c3",
   "metadata": {},
   "source": [
    "## Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df76cc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autoscrolling long output is disabled\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Javascript\n",
    "\n",
    "disable_js = \"\"\"\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "def load_ipython_extension():\n",
    "    display(Javascript(disable_js))\n",
    "    print (\"autoscrolling long output is disabled\")\n",
    "    \n",
    "load_ipython_extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd41d37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Environment =====\n",
      "\n",
      "GPU: Tesla V100-SXM2-16GB\n",
      "\n",
      "PyTorch Configuration\n",
      "   Config         Value\n",
      "-------------  ------------\n",
      "   Version     1.13.0+cu117\n",
      "Built w. CUDA      11.7\n",
      "\n",
      "\n",
      "Other Libraries Configuration\n",
      "  Package       Version                   Commit SHA\n",
      "------------  -----------  ----------------------------------------\n",
      "    epoi        0.1.dev    cd79b19e14a81672f6a62bce7054827352f5eade\n",
      "transformers  4.25.0.dev0  2bdd9fa28411a2822cd1395ed78abeef4a69ec6f\n",
      "  xformers    0.0.14.dev   ba93c5012d00bd1b010514a7bc9bd938c1ad6149\n",
      "  megatron        N/A      0bb597b42c53355a567aba2a1357cc34b9d99ddd\n",
      "   triton        2.0.0                       N/A\n",
      "    apex          0.1                        N/A\n",
      "===== Environment =====\n",
      "\n",
      "[2022-11-10 00:32:33] INFO main: Selected bias_gelu\n",
      "[2022-11-10 00:32:33] INFO main: Selected dropout_add_ln\n",
      "[2022-11-10 00:32:33] INFO main: Selected bert_attention\n",
      "[2022-11-10 00:32:33] INFO main: Selected gpt_attention\n",
      "[2022-11-10 00:32:33] INFO main: Selected qkv_self_attn\n",
      "[2022-11-10 00:32:33] INFO main: Selected t5_attention\n",
      "[2022-11-10 00:32:33] INFO main: Selected layer_norm\n",
      "[2022-11-10 00:32:33] INFO main: Selected softmax\n",
      "[2022-11-10 00:32:33] INFO main: Running selected 8/8 cases\n",
      "[2022-11-10 00:32:33] INFO main: [1/8] Benchmarking bias_gelu\n",
      "[------------------------------------------- Bias+GeLU --------------------------------------------]\n",
      "                        |  Eager (FP32)  |  TS+nvFuser (FP32)  |  Eager (FP16)  |  TS+nvFuser (FP16)\n",
      "1 threads: -----------------------------------------------------------------------------------------\n",
      "      (8, 512, 1024)    |      227.7     |         315.7       |      192.8     |         328.7     \n",
      "      (8, 512, 768)     |      191.0     |         317.3       |      195.0     |         328.7     \n",
      "      (2, 1024, 4096)   |      386.4     |         380.5       |      235.1     |         323.3     \n",
      "      (16, 512, 8192)   |     2624.6     |        2027.9       |     1409.3     |        1082.6     \n",
      "      (16, 512, 32768)  |    10269.7     |        7823.8       |     5439.2     |        3962.1     \n",
      "      (4, 2048, 8192)   |     2628.0     |        2034.6       |     1411.7     |        1082.6     \n",
      "      (4, 2048, 32768)  |    10265.6     |        7821.7       |     5438.8     |        3966.1     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape         Eager (FP32)    TS+nvFuser (FP32)    Eager (FP16)    TS+nvFuser (FP16)\n",
      "----------------  --------------  -------------------  --------------  -------------------\n",
      " (8, 512, 1024)      64.0044            64.0044           48.0024            48.0024\n",
      " (8, 512, 768)       48.0034            48.0034            36.002            36.002\n",
      "(2, 1024, 4096)      128.016            128.016           96.0083            96.0083\n",
      "(16, 512, 8192)        768              592.032             384              336.016\n",
      "(16, 512, 32768)       3072             2176.13             1536             1152.06\n",
      "(4, 2048, 8192)        768              592.032             384              336.016\n",
      "(4, 2048, 32768)       3072             2176.13             1536             1152.06\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:33:15] INFO main: [2/8] Benchmarking dropout_add_ln\n",
      "[------------------------------------- Dropout+Add+LayerNorm --------------------------------------]\n",
      "                        |  Eager (FP32)  |  TS+nvFuser (FP32)  |  Eager (FP16)  |  TS+nvFuser (FP16)\n",
      "1 threads: -----------------------------------------------------------------------------------------\n",
      "      (32, 128, 768)    |      366.7     |         599.9       |      296.2     |         637.1     \n",
      "      (4, 512, 768)     |      298.5     |         592.2       |      292.7     |         631.9     \n",
      "      (8, 512, 1024)    |      459.7     |         660.5       |      300.7     |         672.7     \n",
      "      (64, 128, 1024)   |      833.6     |         790.8       |      472.1     |         677.7     \n",
      "      (16, 512, 8192)   |     6823.3     |        4171.6       |     3572.8     |        2300.0     \n",
      "      (16, 512, 32768)  |    27427.5     |       22598.2       |    14511.4     |       10747.9     \n",
      "      (4, 2048, 8192)   |     6635.5     |        4191.3       |     3570.8     |        2316.7     \n",
      "      (4, 2048, 32768)  |    27635.5     |       22502.1       |    14293.3     |       10767.7     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape         Eager (FP32)    TS+nvFuser (FP32)    Eager (FP16)    TS+nvFuser (FP16)\n",
      "----------------  --------------  -------------------  --------------  -------------------\n",
      " (32, 128, 768)         51              63.0371              27              33.0342\n",
      " (4, 512, 768)          26              32.0215              14              17.0186\n",
      " (8, 512, 1024)         68              84.0391              36              44.0352\n",
      "(64, 128, 1024)        136              168.07               72              88.0664\n",
      "(16, 512, 8192)        1088             1344.12             576              704.094\n",
      "(16, 512, 32768)       4352             6400.34             2304             3328.22\n",
      "(4, 2048, 8192)        1088             1344.12             576              704.094\n",
      "(4, 2048, 32768)       4352             6400.34             2304             3328.22\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:35:29] INFO main: [3/8] Benchmarking bert_attention\n",
      "[2022-11-10 00:35:33] WARNING bencher: Skip correctness checking for xFormers vanilla FlashAttn: Forward failed\n",
      "[2022-11-10 00:35:33] INFO bencher: Correctness checking for xFormers cutlass FlashAttn is passed\n",
      "[2022-11-10 00:35:33] WARNING bencher: Skip correctness checking for xFormers triton FlashAttn: Forward failed\n",
      "[--------- Bert Attention (Attn) and FlashAttention (FA) without mask --------]\n",
      "                                         |  HF (Attn)  |  xFormers cutlass (FA)\n",
      "1 threads: --------------------------------------------------------------------\n",
      "      (8, 512, 1024, 16, 4096, 30522)    |      4.3    |            2.8        \n",
      "      (16, 512, 8192, 64, 32768, 50264)  |    134.9    |          129.5        \n",
      "      (4, 2048, 8192, 64, 32768, 50264)  |    198.3    |          196.0        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "              Shape                 HF (Attn)    xFormers cutlass (FA)\n",
      "---------------------------------  -----------  -----------------------\n",
      " (8, 512, 1024, 16, 4096, 30522)       288               96.25\n",
      "(16, 512, 8192, 64, 32768, 50264)     2816               1026\n",
      "(4, 2048, 8192, 64, 32768, 50264)     8704               2562\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:41:26] INFO main: [4/8] Benchmarking gpt_attention\n",
      "[2022-11-10 00:41:27] INFO bencher: Correctness checking for xFormers FlashAttn (cutlass) is passed\n",
      "[2022-11-10 00:41:27] WARNING bencher: Skip correctness checking for xFormers FlashAttn (triton): Forward failed\n",
      "[----- GPT Attention (Attn) and FlashAttention (FA) without mask ------]\n",
      "                                  |  HF (Attn)  |  xFormers cutlass (FA)\n",
      "1 threads: -------------------------------------------------------------\n",
      "      (8, 1024, 1024, 16, 50257)  |     14.2    |            6.1        \n",
      "      (16, 512, 8192, 64, 50264)  |    184.2    |          164.8        \n",
      "      (4, 2048, 8192, 64, 50264)  |    254.5    |          198.4        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "          Shape              HF (Attn)    xFormers cutlass (FA)\n",
      "--------------------------  -----------  -----------------------\n",
      "(8, 1024, 1024, 16, 50257)     1091              178.502\n",
      "(16, 512, 8192, 64, 50264)    2688.27            1284.02\n",
      "(4, 2048, 8192, 64, 50264)    8836.02            1284.02\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:48:48] INFO main: [5/8] Benchmarking qkv_self_attn\n",
      "[----------------- QKV in Self-Attention ------------------]\n",
      "                           |  NoFuse (FP16)  |  Fused (FP16)\n",
      "1 threads: -------------------------------------------------\n",
      "      (4, 512, 1024, 16)   |       1269.6    |       855.6  \n",
      "      (8, 512, 1024, 16)   |       1369.1    |      1414.6  \n",
      "      (16, 512, 1024, 16)  |       2312.8    |      2641.6  \n",
      "      (16, 512, 8192, 64)  |     110598.2    |    117212.7  \n",
      "      (4, 2048, 8192, 64)  |     110559.7    |    117159.1  \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "       Shape          NoFuse (FP16)    Fused (FP16)\n",
      "-------------------  ---------------  --------------\n",
      "(4, 512, 1024, 16)       42.0063         82.0063\n",
      "(8, 512, 1024, 16)       78.0063         144.006\n",
      "(16, 512, 1024, 16)      150.006         200.006\n",
      "(16, 512, 8192, 64)      1152.05         1376.05\n",
      "(4, 2048, 8192, 64)      1152.05         1376.05\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:52:53] INFO main: [6/8] Benchmarking t5_attention\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-11-10 00:52:54] INFO bencher: Correctness checking for xFormers FlashAttn (native w/o weight scaling) is passed\n",
      "[2022-11-10 00:52:54] INFO layer_ops: Skip correctness checking for CUTLASS and Triton due to not support weight scaling\n",
      "[-------- CrossAttention: T5 (Attn) and FlashAttention (FA) --------]\n",
      "                               |  HF (Attn)  |  xFormers cutlass (FA)\n",
      "1 threads: ----------------------------------------------------------\n",
      "      (4, 1024, 512, 64, 16)   |     10.1    |            5.4        \n",
      "      (4, 1024, 1024, 64, 16)  |     10.6    |            6.0        \n",
      "      (4, 2048, 1024, 64, 16)  |     38.8    |           20.2        \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "         Shape            HF (Attn)    xFormers cutlass (FA)\n",
      "-----------------------  -----------  -----------------------\n",
      "(4, 1024, 512, 64, 16)      1181              292.25\n",
      "(4, 1024, 1024, 64, 16)     1186              296.25\n",
      "(4, 2048, 1024, 64, 16)     4674              1104.5\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:53:43] INFO main: [7/8] Benchmarking layer_norm\n",
      "[2022-11-10 00:53:43] INFO bencher: Correctness checking for Apex (FP16) is passed\n",
      "[2022-11-10 00:53:44] INFO bencher: Correctness checking for Triton (FP16) is passed\n",
      "[2022-11-10 00:53:44] INFO bencher: Correctness checking for xFormers (FP16) is passed\n",
      "[---------------------------------------------------------- LayerNorm ----------------------------------------------------------]\n",
      "                       |  PyTorch (FP32)  |  Apex (FP32)  |  PyTorch (FP16)  |  Apex (FP16)  |  Triton (FP16)  |  xFormers (FP16)\n",
      "1 threads: ----------------------------------------------------------------------------------------------------------------------\n",
      "      (32, 128, 768)   |       265.4      |      308.7    |       198.2      |      255.4    |       741.2     |        877.5    \n",
      "      (8, 512, 1024)   |       309.0      |      353.3    |       217.6      |      265.0    |       742.3     |        862.4    \n",
      "      (16, 512, 8192)  |      4571.1      |     4370.7    |      2371.7      |     2549.4    |      2308.6     |       1690.2    \n",
      "      (4, 2048, 8192)  |      4562.5      |     4368.0    |      2355.5      |     2622.0    |      2285.9     |       1689.7    \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "     Shape        PyTorch (FP32)    Apex (FP32)    PyTorch (FP16)    Apex (FP16)    Triton (FP16)    xFormers (FP16)\n",
      "---------------  ----------------  -------------  ----------------  -------------  ---------------  -----------------\n",
      "(32, 128, 768)       24.0371          24.1309         12.0342          12.1279         12.0342           12.7861\n",
      "(8, 512, 1024)       32.0391          32.1641         16.0352          16.1602         16.0352           17.0371\n",
      "(16, 512, 8192)      512.125          513.125         256.094          257.094         256.094           258.094\n",
      "(4, 2048, 8192)      512.125          513.125         256.094          257.094         256.094           258.094\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n",
      "[2022-11-10 00:54:07] INFO main: [8/8] Benchmarking softmax\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/workspace_hf/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_upper_triang_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/workspace_hf/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_masked_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_masked_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/workspace_hf/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module scaled_softmax_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module scaled_softmax_cuda...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/workspace_hf/Megatron-LM/megatron/fused_kernels/build/build.ninja...\n",
      "Building extension module fused_mix_prec_layer_norm_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_mix_prec_layer_norm_cuda...\n",
      "[2022-11-10 00:54:09] WARNING bencher: Correctness checking for Megatron-LM (Comp-FP32) (backward) is failed: Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 623517 / 16777216 (3.7%)\n",
      "Greatest absolute difference: 0.028654098510742188 at index (0, 1, 995, 329) (up to 0.001 allowed)\n",
      "Greatest relative difference: 1.0081331877729258 at index (0, 1, 995, 870) (up to 0.001 allowed)\n",
      "[2022-11-10 00:54:16] INFO bencher: Correctness checking for xFormers (Comp-FP32) is passed\n",
      "[------------------------------------------------- Softmax with FP16 input -------------------------------------------------]\n",
      "                          |  PyTorch (Comp-FP32)  |  PyTorch (Comp-FP16)  |  Megatron-LM (Comp-FP32)  |  xFormers (Comp-FP32)\n",
      "1 threads: ------------------------------------------------------------------------------------------------------------------\n",
      "      (8, 2, 1024, 1024)  |         1219.4        |         386.2         |           399.3           |         590.9        \n",
      "      (4, 16, 512, 512)   |         1212.3        |         376.0         |           399.3           |         586.5        \n",
      "      (8, 16, 512, 512)   |         2362.1        |         697.1         |           651.2           |         731.6        \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "      Shape          PyTorch (Comp-FP32)    PyTorch (Comp-FP16)    Megatron-LM (Comp-FP32)    xFormers (Comp-FP32)\n",
      "------------------  ---------------------  ---------------------  -------------------------  ----------------------\n",
      "(8, 2, 1024, 1024)           288                    96                       64                        64\n",
      "(4, 16, 512, 512)            288                    96                       64                        64\n",
      "(8, 16, 512, 512)            576                    192                      128                      128\n",
      "\n",
      "Memory is in MBs and excludes inputs/outputs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python3 -m epoi.benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34448faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-bench-env",
   "language": "python",
   "name": "hf-bench-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

[2023-11-07 20:23:51,302] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)
11/07/2023 20:23:52 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True
11/07/2023 20:23:52 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O2,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=/tmp/beans/runs/Nov07_20-23-52_ip-172-31-81-243,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=20,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=/tmp/beans/,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=8,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=False,
report_to=[],
resume_from_checkpoint=None,
run_name=/tmp/beans/,
save_on_each_node=False,
save_safetensors=False,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
/home/ubuntu/pytorch-opt/venv/lib/python3.8/site-packages/datasets/load.py:2086: FutureWarning: 'task' was deprecated in version 2.13.0 and will be removed in 3.0.0.

  warnings.warn(
[INFO|configuration_utils.py:716] 2023-11-07 20:23:52,766 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/config.json
[INFO|configuration_utils.py:776] 2023-11-07 20:23:52,767 >> Model config ResNetConfig {
  "_name_or_path": "microsoft/resnet-50",
  "architectures": [
    "ResNetForImageClassification"
  ],
  "depths": [
    3,
    4,
    6,
    3
  ],
  "downsample_in_bottleneck": false,
  "downsample_in_first_stage": false,
  "embedding_size": 64,
  "finetuning_task": "image-classification",
  "hidden_act": "relu",
  "hidden_sizes": [
    256,
    512,
    1024,
    2048
  ],
  "id2label": {
    "0": "angular_leaf_spot",
    "1": "bean_rust",
    "2": "healthy"
  },
  "label2id": {
    "angular_leaf_spot": "0",
    "bean_rust": "1",
    "healthy": "2"
  },
  "layer_type": "bottleneck",
  "model_type": "resnet",
  "num_channels": 3,
  "out_features": [
    "stage4"
  ],
  "out_indices": [
    4
  ],
  "stage_names": [
    "stem",
    "stage1",
    "stage2",
    "stage3",
    "stage4"
  ],
  "torch_dtype": "float32",
  "transformers_version": "4.35.0.dev0"
}

Downloading pytorch_model.bin:   0%|          | 0.00/103M [00:00<?, ?B/s]Downloading pytorch_model.bin:  10%|█         | 10.5M/103M [00:00<00:06, 14.2MB/s]Downloading pytorch_model.bin:  31%|███       | 31.5M/103M [00:01<00:02, 27.7MB/s]Downloading pytorch_model.bin:  41%|████      | 41.9M/103M [00:01<00:02, 28.0MB/s]Downloading pytorch_model.bin:  61%|██████▏   | 62.9M/103M [00:02<00:01, 30.2MB/s]Downloading pytorch_model.bin:  72%|███████▏  | 73.4M/103M [00:02<00:00, 36.2MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 83.9M/103M [00:02<00:00, 29.5MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 94.4M/103M [00:03<00:00, 26.6MB/s]Downloading pytorch_model.bin: 100%|██████████| 103M/103M [00:03<00:00, 24.7MB/s] Downloading pytorch_model.bin: 100%|██████████| 103M/103M [00:03<00:00, 26.9MB/s]
[INFO|modeling_utils.py:3013] 2023-11-07 20:23:56,844 >> loading weights file pytorch_model.bin from cache at /home/ubuntu/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/pytorch_model.bin
[INFO|modeling_utils.py:3803] 2023-11-07 20:23:57,189 >> All model checkpoint weights were used when initializing ResNetForImageClassification.

[WARNING|modeling_utils.py:3824] 2023-11-07 20:23:57,189 >> Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:
- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([3, 2048]) in the model instantiated
- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([3]) in the model instantiated
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Downloading (…)rocessor_config.json:   0%|          | 0.00/266 [00:00<?, ?B/s]Downloading (…)rocessor_config.json: 100%|██████████| 266/266 [00:00<00:00, 162kB/s]
[INFO|image_processing_utils.py:369] 2023-11-07 20:23:57,556 >> loading configuration file preprocessor_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--microsoft--resnet-50/snapshots/4067a2728b9c93fbd67b9d5a30b03495ac74a46e/preprocessor_config.json
[WARNING|image_processing_auto.py:358] 2023-11-07 20:23:57,556 >> Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.
[INFO|image_processing_utils.py:732] 2023-11-07 20:23:57,558 >> size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 224. Converted to {'shortest_edge': 224}.
[INFO|image_processing_utils.py:419] 2023-11-07 20:23:57,559 >> Image processor ConvNextImageProcessor {
  "crop_pct": 0.875,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.485,
    0.456,
    0.406
  ],
  "image_processor_type": "ConvNextImageProcessor",
  "image_std": [
    0.229,
    0.224,
    0.225
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}

[INFO|trainer.py:535] 2023-11-07 20:23:58,593 >> max_steps is given, it will override any value given in num_train_epochs
[INFO|trainer.py:584] 2023-11-07 20:23:58,593 >> Using auto half precision backend
/home/ubuntu/pytorch-opt/transformers-benchmarks/transformers/src/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1669] 2023-11-07 20:23:59,191 >> ***** Running training *****
[INFO|trainer.py:1670] 2023-11-07 20:23:59,191 >>   Num examples = 1,034
[INFO|trainer.py:1671] 2023-11-07 20:23:59,191 >>   Num Epochs = 1
[INFO|trainer.py:1672] 2023-11-07 20:23:59,191 >>   Instantaneous batch size per device = 8
[INFO|trainer.py:1675] 2023-11-07 20:23:59,191 >>   Total train batch size (w. parallel, distributed & accumulation) = 8
[INFO|trainer.py:1676] 2023-11-07 20:23:59,191 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1677] 2023-11-07 20:23:59,191 >>   Total optimization steps = 20
[INFO|trainer.py:1678] 2023-11-07 20:23:59,192 >>   Number of trainable parameters = 23,514,179
  0%|          | 0/20 [00:00<?, ?it/s][W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  5%|▌         | 1/20 [00:03<01:04,  3.42s/it] 10%|█         | 2/20 [00:03<00:26,  1.49s/it] 15%|█▌        | 3/20 [00:03<00:14,  1.15it/s] 20%|██        | 4/20 [00:03<00:09,  1.73it/s] 25%|██▌       | 5/20 [00:03<00:06,  2.35it/s] 30%|███       | 6/20 [00:04<00:04,  3.01it/s] 35%|███▌      | 7/20 [00:04<00:03,  3.72it/s] 40%|████      | 8/20 [00:04<00:02,  4.39it/s] 45%|████▌     | 9/20 [00:04<00:02,  4.99it/s] 50%|█████     | 10/20 [00:04<00:01,  5.54it/s] 55%|█████▌    | 11/20 [00:04<00:01,  5.94it/s] 60%|██████    | 12/20 [00:04<00:01,  6.30it/s] 65%|██████▌   | 13/20 [00:05<00:01,  6.54it/s] 70%|███████   | 14/20 [00:05<00:00,  6.78it/s] 75%|███████▌  | 15/20 [00:05<00:00,  6.90it/s] 80%|████████  | 16/20 [00:05<00:00,  6.94it/s] 85%|████████▌ | 17/20 [00:05<00:00,  7.10it/s] 90%|█████████ | 18/20 [00:05<00:00,  7.20it/s] 95%|█████████▌| 19/20 [00:05<00:00,  7.26it/s]100%|██████████| 20/20 [00:06<00:00,  7.32it/s][INFO|trainer.py:1905] 2023-11-07 20:24:05,242 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                               {'train_runtime': 6.0498, 'train_samples_per_second': 26.447, 'train_steps_per_second': 3.306, 'step_time_list': [3.282651662826538, 0.06703901290893555, 0.06705665588378906, 0.06716775894165039, 0.09299492835998535, 0.08434820175170898, 0.0825800895690918, 0.07865118980407715, 0.07863926887512207, 0.07821536064147949, 0.0794839859008789, 0.07751154899597168, 0.07938075065612793, 0.07796382904052734, 0.07684516906738281, 0.07818365097045898, 0.07643461227416992, 0.07522034645080566, 0.07431864738464355, 0.07511353492736816], 'train_loss': 1.1402816772460938, 'init_mem_cpu_alloc_delta': 897359872, 'init_mem_gpu_alloc_delta': 94304256, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': 1542246400, 'train_mem_gpu_alloc_delta': 314590720, 'train_mem_cpu_peaked_delta': 0, 'train_mem_gpu_peaked_delta': 432229888, 'before_init_mem_cpu': 736964608, 'before_init_mem_gpu': 0, 'epoch': 0.15}
100%|██████████| 20/20 [00:06<00:00,  7.32it/s]100%|██████████| 20/20 [00:06<00:00,  3.24it/s]
[INFO|trainer.py:2810] 2023-11-07 20:24:05,374 >> Saving model checkpoint to /tmp/beans/
[INFO|configuration_utils.py:461] 2023-11-07 20:24:05,375 >> Configuration saved in /tmp/beans/config.json
[INFO|modeling_utils.py:2121] 2023-11-07 20:24:05,512 >> Model weights saved in /tmp/beans/pytorch_model.bin
[INFO|image_processing_utils.py:253] 2023-11-07 20:24:05,513 >> Image processor saved in /tmp/beans/preprocessor_config.json
***** train metrics *****
  before_init_mem_cpu        =                                                                                                                                    702MB
  before_init_mem_gpu        =                                                                                                                                      0MB
  epoch                      =                                                                                                                                     0.15
  init_mem_cpu_alloc_delta   =                                                                                                                                    855MB
  init_mem_cpu_peaked_delta  =                                                                                                                                      0MB
  init_mem_gpu_alloc_delta   =                                                                                                                                     89MB
  init_mem_gpu_peaked_delta  =                                                                                                                                      0MB
  step_time_list             = 3.2827,0.067,0.0671,0.0672,0.093,0.0843,0.0826,0.0787,0.0786,0.0782,0.0795,0.0775,0.0794,0.078,0.0768,0.0782,0.0764,0.0752,0.0743,0.0751
  train_loss                 =                                                                                                                                   1.1403
  train_mem_cpu_alloc_delta  =                                                                                                                                   1470MB
  train_mem_cpu_peaked_delta =                                                                                                                                      0MB
  train_mem_gpu_alloc_delta  =                                                                                                                                    300MB
  train_mem_gpu_peaked_delta =                                                                                                                                    412MB
  train_runtime              =                                                                                                                               0:00:06.04
  train_samples_per_second   =                                                                                                                                   26.447
  train_steps_per_second     =                                                                                                                                    3.306
[INFO|modelcard.py:452] 2023-11-07 20:24:05,544 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Image Classification', 'type': 'image-classification'}, 'dataset': {'name': 'beans', 'type': 'beans', 'config': 'default', 'split': 'train', 'args': 'default'}}
